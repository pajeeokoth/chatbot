{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4efdd0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4817f70d",
   "metadata": {},
   "source": [
    "# Fly Me\n",
    "A travel provider for individuals and professionals. \n",
    "\n",
    "## Overview\n",
    "The chatbot project - Jupyter Notebook\n",
    "Fly Me has launched an ambitious project to develop a *chatbot* to *help users choose a travel offer*.\n",
    "\n",
    "The first phase of this project is to **develop an MVP** that will help Fly Me employees to easily book airline tickets for their holidays.\n",
    "\n",
    "This first MVP will allow us to test the concept and performance of the chatbot quickly and on a large scale.\n",
    "\n",
    "As this project is iterative, we have limited the features of the chatbot V1. It must be able to identify the following five elements in the user's request:\n",
    "\n",
    "* Departure city\n",
    "* Destination city\n",
    "* Desired flight departure date\n",
    "* Desired flight return date\n",
    "* Maximum budget for the total price of tickets\n",
    "\n",
    "If one of the elements is missing, the chatbot must be able to ask the user the relevant questions in order to fully understand the request. When the chatbot thinks it has understood all the elements of the user's request, it must be able to reformulate the user's request and ask the user for confirmation.\n",
    "\n",
    "## Tools and Technologies used\n",
    "To carry out this project, we will need to use the following tools and technologies:\n",
    "\n",
    "* The Microsoft Bot Framework source code for Python ‚ÄúMicrosoft Bot Framework SDK v4 for Python‚Äù \n",
    "* The Azure LUIS Cognitive Service which allows you to perform a semantic analysis of a message entered by the user and structure it for processing by the bot (it should allow you to identify the five elements requested)\n",
    "* The Azure Web App service that allows you to run a web application on the Azure Cloud (you won't need to use the Azure Bot service)\n",
    "* The Bot Framework Emulator which will allow you to test your chatbot locally and in production. It is an interface that allows a user to interact with the chatbot.\n",
    "## Data\n",
    "The data used to train the LUIS model comes from a dataset of conversations between users and travel agents. This dataset is in JSON format and contains a total of 1500 conversations. Each conversation consists of a series of messages exchanged between the user and the travel agent. The messages contain information about the user's travel preferences, such as departure city, destination city, travel dates, and budget.\n",
    "The dataset is divided into two parts: a training set and a test set. The training set contains 1200 conversations and is used to train the LUIS model. The test set contains 300 conversations and is used to evaluate the performance of the LUIS model.\n",
    "The dataset is available in the `data/frames_dataset` directory of the project. The main file is `frames.json`, which contains all the conversations. The file is structured as follows:\n",
    "```json\n",
    "{\n",
    "  \"conversations\": [\n",
    "    {\n",
    "      \"id\": \"1\",\n",
    "      \"turns\": [\n",
    "        {\n",
    "          \"speaker\": \"user\",\n",
    "          \"text\": \"I want to book a flight from New York to Paris.\"\n",
    "        },\n",
    "        {\n",
    "          \"speaker\": \"agent\",\n",
    "          \"text\": \"Sure, when do you want to depart?\"\n",
    "        },\n",
    "        ...\n",
    "      ]\n",
    "    },\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "```\n",
    "The `turns` field contains a list of messages exchanged between the user and the travel agent. Each message has a `speaker` field indicating who sent the message (either \"user\" or \"agent\") and a `text` field containing the content of the message.\n",
    "## Objective\n",
    "The objective of this notebook is to explore and preprocess the dataset to prepare it for training the LUIS model. This includes loading the JSON file, flattening the nested structure, and extracting relevant information from the conversations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c6b50e",
   "metadata": {},
   "source": [
    "## What is a bot?\n",
    "Bots provide an experience that feels less like using a computer and more like dealing with a person‚Äîor intelligent robot. You can use bots to shift simple, repetitive tasks‚Äîsuch as taking a dinner reservation or gathering profile information‚Äîonto automated systems that may no longer require direct human intervention. Users converse with a bot using text, interactive cards, and speech. A bot interaction can be a quick answer to a question or an involved conversation that intelligently provides access to services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90555fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened DataFrame shape: (19986, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>wizard_id</th>\n",
       "      <th>id</th>\n",
       "      <th>labels.userSurveyRating</th>\n",
       "      <th>labels.wizardSurveyTaskSuccessful</th>\n",
       "      <th>turns.text</th>\n",
       "      <th>turns.author</th>\n",
       "      <th>turns.timestamp</th>\n",
       "      <th>turns.labels.acts</th>\n",
       "      <th>turns.labels.acts_without_refs</th>\n",
       "      <th>turns.labels.active_frame</th>\n",
       "      <th>turns.labels.frames</th>\n",
       "      <th>turns.db.result</th>\n",
       "      <th>turns.db.search</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U22HTHYNP</td>\n",
       "      <td>U21DKG18C</td>\n",
       "      <td>e2c0fc6c-2134-4891-8353-ef16d8412c9a</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>I'd like to book a trip to Atlantis from Capri...</td>\n",
       "      <td>user</td>\n",
       "      <td>1.471272e+12</td>\n",
       "      <td>[{'args': [{'val': 'book', 'key': 'intent'}], ...</td>\n",
       "      <td>[{'args': [{'val': 'book', 'key': 'intent'}], ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'info': {'intent': [{'val': 'book', 'negated...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U22HTHYNP</td>\n",
       "      <td>U21DKG18C</td>\n",
       "      <td>e2c0fc6c-2134-4891-8353-ef16d8412c9a</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>Hi...I checked a few options for you, and unfo...</td>\n",
       "      <td>wizard</td>\n",
       "      <td>1.471272e+12</td>\n",
       "      <td>[{'args': [{'val': [{'annotations': [], 'frame...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'info': {'intent': [{'val': 'book', 'negated...</td>\n",
       "      <td>[[{'trip': {'returning': {'duration': {'hours'...</td>\n",
       "      <td>[{'ORIGIN_CITY': 'Porto Alegre', 'PRICE_MIN': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U22HTHYNP</td>\n",
       "      <td>U21DKG18C</td>\n",
       "      <td>e2c0fc6c-2134-4891-8353-ef16d8412c9a</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>Yes, how about going to Neverland from Caprica...</td>\n",
       "      <td>user</td>\n",
       "      <td>1.471273e+12</td>\n",
       "      <td>[{'args': [{'val': 'Neverland', 'key': 'dst_ci...</td>\n",
       "      <td>[{'args': [{'val': 'Neverland', 'key': 'dst_ci...</td>\n",
       "      <td>2</td>\n",
       "      <td>[{'info': {'intent': [{'val': 'book', 'negated...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id  wizard_id                                    id  \\\n",
       "0  U22HTHYNP  U21DKG18C  e2c0fc6c-2134-4891-8353-ef16d8412c9a   \n",
       "1  U22HTHYNP  U21DKG18C  e2c0fc6c-2134-4891-8353-ef16d8412c9a   \n",
       "2  U22HTHYNP  U21DKG18C  e2c0fc6c-2134-4891-8353-ef16d8412c9a   \n",
       "\n",
       "   labels.userSurveyRating  labels.wizardSurveyTaskSuccessful  \\\n",
       "0                      4.0                               True   \n",
       "1                      4.0                               True   \n",
       "2                      4.0                               True   \n",
       "\n",
       "                                          turns.text turns.author  \\\n",
       "0  I'd like to book a trip to Atlantis from Capri...         user   \n",
       "1  Hi...I checked a few options for you, and unfo...       wizard   \n",
       "2  Yes, how about going to Neverland from Caprica...         user   \n",
       "\n",
       "   turns.timestamp                                  turns.labels.acts  \\\n",
       "0     1.471272e+12  [{'args': [{'val': 'book', 'key': 'intent'}], ...   \n",
       "1     1.471272e+12  [{'args': [{'val': [{'annotations': [], 'frame...   \n",
       "2     1.471273e+12  [{'args': [{'val': 'Neverland', 'key': 'dst_ci...   \n",
       "\n",
       "                      turns.labels.acts_without_refs  \\\n",
       "0  [{'args': [{'val': 'book', 'key': 'intent'}], ...   \n",
       "1                                                NaN   \n",
       "2  [{'args': [{'val': 'Neverland', 'key': 'dst_ci...   \n",
       "\n",
       "   turns.labels.active_frame  \\\n",
       "0                          1   \n",
       "1                          1   \n",
       "2                          2   \n",
       "\n",
       "                                 turns.labels.frames  \\\n",
       "0  [{'info': {'intent': [{'val': 'book', 'negated...   \n",
       "1  [{'info': {'intent': [{'val': 'book', 'negated...   \n",
       "2  [{'info': {'intent': [{'val': 'book', 'negated...   \n",
       "\n",
       "                                     turns.db.result  \\\n",
       "0                                                NaN   \n",
       "1  [[{'trip': {'returning': {'duration': {'hours'...   \n",
       "2                                                NaN   \n",
       "\n",
       "                                     turns.db.search  \n",
       "0                                                NaN  \n",
       "1  [{'ORIGIN_CITY': 'Porto Alegre', 'PRICE_MIN': ...  \n",
       "2                                                NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Flatten nested JSON columns into a flat DataFrame\n",
    "from pandas import json_normalize\n",
    "# Load JSON file\n",
    "path = '../data/frames_dataset/frames.json'\n",
    "with open(path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# create a flat DataFrame from the loaded `data` variable\n",
    "try:\n",
    "    # Normalize `data` to have all dicts/lists at the same level\n",
    "    if 'data' in globals() and (isinstance(data, list) or isinstance(data, dict)):\n",
    "        df_flat = json_normalize(data, sep='.')\n",
    "    else:\n",
    "        # Fall back to normalizing rows from the existing DataFrame `df`\n",
    "        df_flat = json_normalize(df.to_dict(orient='records'), sep='.')\n",
    "except Exception as e:\n",
    "    print('Normalization error:', e)\n",
    "    df_flat = df.copy()\n",
    "\n",
    "# Handle columns that are lists of dicts: detect and explode+normalize them\n",
    "for col in df_flat.columns.tolist():\n",
    "    # If the column contains lists of dicts, expand them\n",
    "    if df_flat[col].apply(lambda x: isinstance(x, list) and len(x) > 0 and isinstance(x[0], dict)).any():\n",
    "        # Explode the list so each element becomes its own row, then normalize that column\n",
    "        exploded = df_flat.explode(col).reset_index(drop=True)\n",
    "        # Normalize the exploded column (it may contain dicts or NaN)\n",
    "        expanded = json_normalize(exploded[col].dropna().tolist(), sep='.')\n",
    "        # Prefix new columns with the original column name\n",
    "        expanded = expanded.add_prefix(col + '.')\n",
    "        # Join back to exploded frame (align by index)\n",
    "        exploded = exploded.drop(columns=[col]).join(expanded)\n",
    "        df_flat = exploded\n",
    "\n",
    "# Show results\n",
    "print('Flattened DataFrame shape:', df_flat.shape)\n",
    "display(df_flat.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fbf2df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"I'd like to book a trip to Atlantis from Caprica on Saturday, August 13, 2016 for 8 adults. I have a tight budget of 1700.\",\n",
       "       'Hi...I checked a few options for you, and unfortunately, we do not currently have any trips that meet this criteria.  Would you like to book an alternate travel option?',\n",
       "       'Yes, how about going to Neverland from Caprica on August 13, 2016 for 5 adults. For this trip, my budget would be 1900.',\n",
       "       ..., 'Consider it done! Have a good trip :slightly_smiling_face:',\n",
       "       'Thanks!', 'My pleasure!'], shape=(19986,), dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flat['turns.text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "879b98be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64bit\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(platform.architecture()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa8d399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = df_flat.sample(5, random_state=42)\n",
    "# json_string = sample.to_json()\n",
    "# print(json_string)\n",
    "# # Save the flattened DataFrame to a CSV file\n",
    "# sample.to_json('../data/frames_dataset/sample_frames.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b97de2",
   "metadata": {},
   "source": [
    "## Create Conversation Analysis Client\n",
    "To create a Conversation Analysis client, you need to install the `azure-ai-conversationanalysis` package. You can do this using pip:\n",
    "\n",
    "```bashpip install azure-ai-conversationanalysis\n",
    "```\n",
    "```## Import necessary libraries\n",
    "```python\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "# Load the JSON file\n",
    "with open('../data/frames_dataset/frames.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "# Normalize the JSON data to flatten the structure\n",
    "df = json_normalize(data['conversations'], 'turns', ['id'])\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff433159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.core\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.language.conversations import ConversationAnalysisClient\n",
    "\n",
    "endpoint = \"https://mytravel.cognitiveservices.azure.com/\"\n",
    "credential = AzureKeyCredential(\"key1\")\n",
    "client = ConversationAnalysisClient(endpoint, credential)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475fec50",
   "metadata": {},
   "source": [
    "#### Generate a LUIS JSON from the frames.json file\n",
    "This dataset is in JSON format and contains a total of 1500 conversations. Each conversation consists of a series of messages exchanged between the user and the travel agent. The messages contain information about the user's travel preferences, such as departure city, destination city, travel dates, and budget.\n",
    "The dataset is divided into two parts: a training set and a test set. The training set contains 1200 conversations and is used to train the LUIS model. The test set contains 300 conversations and is used to evaluate the performance of the LUIS model.\n",
    "The dataset is available in the `data/frames_dataset` directory of the project. The main file is `frames.json`, which contains all the conversations. The file is structured as follows:\n",
    "```json\n",
    "{\n",
    "  \"conversations\": [\n",
    "    {\n",
    "      \"id\": \"1\",\n",
    "      \"turns\": [\n",
    "        {\n",
    "          \"speaker\": \"user\",\n",
    "          \"text\": \"I want to book a flight from New York to Paris.\"\n",
    "        },\n",
    "        {\n",
    "          \"speaker\": \"agent\",\n",
    "          \"text\": \"Sure, when do you want to depart?\"\n",
    "        },\n",
    "        ...\n",
    "      ]\n",
    "    },\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "```\n",
    "The dataset contains conversation in French and English. The LUIS model must be able to understand both languages.\n",
    "Each conversation is identified by a unique ID and consists of multiple turns. Each turn has a speaker (either \"user\" or \"agent\") and the text of the message.\n",
    "The structure:\n",
    "* multiple turns per conversation (user and agent)\n",
    "* each turn has a speaker and text (frames)\n",
    "* information about travel preferences (departure city, destination city, travel dates, budget)\n",
    "The goal is to extract the relevant information (<user utterances>) from the conversations and format it according to the LUIS JSON structure. This includes identifying the intents and entities in the user's messages and structuring (map slots/entities) them in a way that LUIS can understand.\n",
    "\n",
    "**LUIS JSON Structure:**\n",
    "```json\n",
    "{\n",
    "  \"luis_schema_version\": \"7.0.0\",\n",
    "  \"versionId\": \"0.1\",\n",
    "  \"name\": \"TravelBooking\",\n",
    "  \"desc\": \"LUIS model for travel booking chatbot\",\n",
    "  \"culture\": \"en-us\",\n",
    "  \"intents\": [\n",
    "    {\n",
    "      \"name\": \"BookFlight\"\n",
    "    }\n",
    "  ],\n",
    "  \"entities\": [\n",
    "    {\n",
    "      \"name\": \"DepartureCity\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"DestinationCity\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"DepartureDate\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"ReturnDate\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Budget\"\n",
    "    }\n",
    "  ],\n",
    "  \"composites\": [],\n",
    "  \"closedLists\": [],\n",
    "  \"patternAnyEntities\": [],\n",
    "  \"regex_entities\": [],\n",
    "  \"prebuiltEntities\": [],\n",
    "  \"model_features\": [],\n",
    "  \"regex_features\": [],\n",
    "  \"utterances\": [\n",
    "    {\n",
    "      \"text\": \"I want to book a flight from New York to Paris.\",\n",
    "      \"intent\": \"BookFlight\",\n",
    "      \"entities\": [\n",
    "        {\n",
    "          \"entity\": \"DepartureCity\",\n",
    "          \"startPos\": 27,\n",
    "          \"endPos\": 34\n",
    "        },\n",
    "        {\n",
    "          \"entity\": \"DestinationCity\",\n",
    "          \"startPos\": 38,\n",
    "          \"endPos\": 42\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"patterns\": []\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eee0792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date-first extraction using dateparser (prefer these spans)\n",
    "import dateparser\n",
    "import re\n",
    "\n",
    "MONTH_WORDS = r'\\b(?:jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec|january|february|march|april|may|june|july|august|september|october|november|december)\\b'\n",
    "\n",
    "def find_date_spans(text):\n",
    "    \"\"\"\n",
    "    Return list of (start, end, matched_text, parsed_dt) found by dateparser.search.search_dates.\n",
    "    Fall back to a few explicit regexes (ISO) if needed.\n",
    "    \"\"\"\n",
    "    spans = []\n",
    "    # try dateparser search first\n",
    "    try:\n",
    "        res = dateparser.search.search_dates(text, settings={'PREFER_DATES_FROM': 'future', 'RETURN_AS_TIMEZONE_AWARE': False})\n",
    "    except Exception:\n",
    "        res = None\n",
    "    if res:\n",
    "        for match_text, dt in res:\n",
    "            # find first occurrence of match_text (case-insensitive) and use it\n",
    "            m = re.search(re.escape(match_text), text, flags=re.IGNORECASE)\n",
    "            if m:\n",
    "                spans.append((m.start(), m.end()-1, match_text, dt))\n",
    "    # fallback: explicit ISO-ish regex spans not caught by dateparser\n",
    "    for m in re.finditer(r'\\b20\\d{2}[/-]\\d{1,2}[/-]\\d{1,2}\\b', text):\n",
    "        spans.append((m.start(), m.end()-1, text[m.start():m.end()], None))\n",
    "    return spans\n",
    "\n",
    "\n",
    "# Improved numeric-as-budget decision (use date spans + context cues + year heuristics)\n",
    "CURRENCY_CUES = ['$', 'usd', 'dollars', 'eur', '‚Ç¨', '¬£', 'budget', 'price', 'cost', 'fare', 'ticket', 'pay']\n",
    "\n",
    "def is_token_overlapping_spans(start, end, spans):\n",
    "    return any(s <= start <= e or s <= end <= e for (s,e, *_ ) in spans)\n",
    "\n",
    "def is_likely_budget_token(text, start, end, date_spans):\n",
    "    \"\"\"\n",
    "    Return True if numeric token at (start,end) is likely a budget, False otherwise.\n",
    "    Uses date_spans to avoid classifying date tokens.\n",
    "    \"\"\"\n",
    "    # don't label if overlaps an already-detected date\n",
    "    if is_token_overlapping_spans(start, end, date_spans):\n",
    "        return False\n",
    "\n",
    "    window = text[max(0, start-30):min(len(text), end+30)].lower()\n",
    "\n",
    "    # strong currency cues -> budget\n",
    "    if any(cue in window for cue in CURRENCY_CUES):\n",
    "        return True\n",
    "    # currency symbol immediately before e.g. $1900\n",
    "    if start > 0 and text[start-1] in ['$', '‚Ç¨', '¬£']:\n",
    "        return True\n",
    "\n",
    "    token = text[start:end+1]\n",
    "    # numeric-only value\n",
    "    try:\n",
    "        val = int(re.sub(r'[^0-9]', '', token))\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "    # heuristic rules for years vs price:\n",
    "    # if token is a 4-digit year within typical year range and there's a month word nearby -> treat as date\n",
    "    if len(token) == 4 and 1900 <= val <= 2035:\n",
    "        month_nearby = re.search(MONTH_WORDS, window, flags=re.IGNORECASE)\n",
    "        if month_nearby:\n",
    "            return False  # likely a year in date context\n",
    "        # if sentence contains explicit date tokens (24, Aug etc) treat as Date\n",
    "        if re.search(r'\\b\\d{1,2}\\b', window) and re.search(MONTH_WORDS, window, flags=re.IGNORECASE):\n",
    "            return False\n",
    "\n",
    "    # numeric-value heuristics: consider values >=100 as plausible budgets (tunable)\n",
    "    if val >= 100 and val <= 1000000:\n",
    "        # If text contains words like 'on' before token plus month words, that might be a date: be conservative\n",
    "        before = text[max(0, start-10):start].lower()\n",
    "        if re.search(r'\\bon\\b', before) and re.search(MONTH_WORDS, text, flags=re.IGNORECASE):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6d593ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LUIS JSON created successfully!\n",
      "üìÑ Saved to: ../data/frames_dataset/luis_flight_booking.json\n",
      "üß† Intents: 1, Entities: 10, Utterances: 10407\n"
     ]
    }
   ],
   "source": [
    "# Using spaCy NER extraction of LUIS utterances from frames.json\n",
    "# - Uses spaCy NER when available (fallback to rule-based regex and heuristics)\n",
    "# - Adds more entity patterns (airport codes, times, currencies, passenger counts)\n",
    "\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# === Configuration, Reuse existing notebook constants if available, else fall back ===\n",
    "INPUT_FILE = globals().get('INPUT_FILE', '../data/frames_dataset/frames.json')\n",
    "OUTPUT_FILE = globals().get('OUTPUT_FILE', '../data/frames_dataset/luis_flight_booking.json')\n",
    "\n",
    "# === Intent mapping from frame actions ===\n",
    "INTENT_MAP = globals().get('INTENT_MAP', {\n",
    "    'book': 'BookFlight',\n",
    "    'inform': 'ProvideInfo',\n",
    "    'offer': 'OfferFlight',\n",
    "    'request': 'RequestInfo',\n",
    "    'confirm': 'ConfirmBooking',\n",
    "    'greet': 'Greet',\n",
    "    'thankyou': 'ThankYou',\n",
    "    'select': 'SelectOption',\n",
    "    'deny': 'DenyRequest',\n",
    "    'ack': 'Acknowledge'\n",
    "})\n",
    "\n",
    "# === Entity normalization mapping ===\n",
    "ENTITY_MAP = globals().get('ENTITY_MAP', {\n",
    "    'departure_city': 'DepartureCity',\n",
    "    'from_city': 'DepartureCity',\n",
    "    'origin_city': 'DepartureCity',\n",
    "    'arrival_city': 'DestinationCity',\n",
    "    'to_city': 'DestinationCity',\n",
    "    'destination_city': 'DestinationCity',\n",
    "    'depart_date': 'DepartureDate',\n",
    "    'return_date': 'ReturnDate',\n",
    "    'date': 'Date',\n",
    "    'price': 'Budget',\n",
    "    'budget': 'Budget',\n",
    "    'num_people': 'NumPassengers'\n",
    "})\n",
    "\n",
    "# === Blocklist certain spaCy labels and unwanted entity names ===\n",
    "BLOCKLIST = {'EVENT', 'FAC', 'LAW', 'PRODUCT', 'NORP', 'PERCENT', 'PERSON', 'WORK_OF_ART'}\n",
    "\n",
    "# === Try to import spaCy and add rule-based matcher if available ===\n",
    "USE_SPACY = False\n",
    "try:\n",
    "    import spacy\n",
    "    from spacy.matcher import Matcher\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    USE_SPACY = True\n",
    "except Exception:\n",
    "    print('spaCy not available or model missing, falling back to rule-based extraction', file=sys.stderr)\n",
    "\n",
    "# === Position finder: finds the best case-insensitive span for a value in text ===\n",
    "def find_positions(text, value):\n",
    "    if not value or not text:\n",
    "        return None\n",
    "    s = str(value).strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    # === direct case-insensitive search first ===\n",
    "    m = re.search(re.escape(s), text, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        return m.start(), m.end() - 1\n",
    "\n",
    "    # === normalized whitespace and punctuation relaxed search ===\n",
    "    norm_text = re.sub(r'\\s+', ' ', text).lower()\n",
    "    norm_value = re.sub(r'\\s+', ' ', s).lower()\n",
    "    idx = norm_text.find(norm_value)\n",
    "    if idx != -1:\n",
    "        words = norm_value.split()\n",
    "        pattern = r'\\\\b' + r'\\\\s+'.join(map(re.escape, words)) + r'\\\\b'\n",
    "        m2 = re.search(pattern, text, flags=re.IGNORECASE)\n",
    "        if m2:\n",
    "            return m2.start(), m2.end() - 1\n",
    "\n",
    "    # === fuzzy word match fallback ===\n",
    "    from difflib import get_close_matches\n",
    "    words = norm_text.split()\n",
    "    cm = get_close_matches(norm_value, words, n=1, cutoff=0.8)\n",
    "    if cm:\n",
    "        word = cm[0]\n",
    "        m3 = re.search(re.escape(word), text, flags=re.IGNORECASE)\n",
    "        if m3:\n",
    "            return m3.start(), m3.end() - 1\n",
    "    return None\n",
    "\n",
    "# === Rule-based regex patterns to capture common entities ===\n",
    "ENTITY_PATTERNS = [\n",
    "    (\"DepartureDate\", re.compile(r\"\\b(20\\d{2}[\\/-]\\d{1,2}[\\/-]\\d{1,2})\\b\")),\n",
    "    (\"DepartureDate\", re.compile(r\"\\b(\\d{1,2}\\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Sept|Oct|Nov|Dec)[a-z]*\\s+20\\d{2})\\b\", re.IGNORECASE)),\n",
    "    (\"Time\", re.compile(r\"\\b(\\d{1,2}:(?:\\d{2})(?:\\s?[APMapm]{2})?)\\b\")),\n",
    "    (\"Budget\", re.compile(r\"\\b\\$\\s?\\d{1,3}(?:,\\d{3})*(?:\\.\\d{1,2})?\\b\")),\n",
    "    (\"Budget\", re.compile(r\"\\b(?:usd|dollars|eur|‚Ç¨|¬£)\\s?\\d{1,3}(?:,\\d{3})*(?:\\.\\d{1,2})?\\b\", re.IGNORECASE)),\n",
    "    # keep AirportCode and NumPassengers patterns; numeric-only budgets handled separately below\n",
    "    (\"AirportCode\", re.compile(r\"\\b[A-Z]{3}\\b\")),\n",
    "    (\"NumPassengers\", re.compile(r\"\\b(\\d+)\\s*(?:passengers|people|persons|pax)\\b\", re.IGNORECASE)),\n",
    "]\n",
    "\n",
    "# === Add spaCy matcher patterns ===\n",
    "if USE_SPACY:\n",
    "    try:\n",
    "        matcher.add('NUM_PASS', [[{\"IS_DIGIT\": True}, {\"LOWER\": {\"IN\": [\"adult\", \"adults\", \"passenger\", \"passengers\"]}}]])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# === helper to apply spaCy NER and also our rule-based patterns ===\n",
    "def extract_entities_from_text(text):\n",
    "    date_spans = find_date_spans(text)  # list of (s,e,text,dt)\n",
    "    entities = []\n",
    "    # === add date entities first ===\n",
    "    for s, e, match, dt in date_spans:\n",
    "        entities.append({'entity': 'Date', 'startPos': s, 'endPos': e, 'text': match})\n",
    "\n",
    "    # === apply rule-based patterns once (avoid duplicates and date overlaps) ===\n",
    "    for etype, pattern in ENTITY_PATTERNS:\n",
    "        try:\n",
    "            for m in pattern.finditer(text):\n",
    "                s, e = m.start(), m.end() - 1\n",
    "                if is_token_overlapping_spans(s, e, [(ds, de) for ds, de, _, _ in date_spans]):\n",
    "                    continue\n",
    "                if any(ent['startPos'] <= s <= ent['endPos'] or ent['startPos'] <= e <= ent['endPos'] for ent in entities):\n",
    "                    continue\n",
    "                entities.append({'entity': etype, 'startPos': s, 'endPos': e, 'text': text[s:e+1]})\n",
    "        except re.error:\n",
    "            continue\n",
    "\n",
    "    # === now numeric-only tokens (e.g. \"1900\") - use the date-aware budget detector ===\n",
    "    for m in re.finditer(r\"\\b\\d{3,6}\\b\", text):\n",
    "        s, e = m.start(), m.end() - 1\n",
    "        # === skip numbers overlapping date spans or already captured ===\n",
    "        if is_token_overlapping_spans(s, e, [(ds, de) for ds, de, _, _ in date_spans]):\n",
    "            continue\n",
    "        if any(ent['startPos'] <= s <= ent['endPos'] or ent['startPos'] <= e <= ent['endPos'] for ent in entities):\n",
    "            continue\n",
    "        if is_likely_budget_token(text, s, e, date_spans):\n",
    "            span_text = text[s:e+1]\n",
    "            entities.append({'entity': 'Budget', 'startPos': s, 'endPos': e, 'text': span_text})\n",
    "\n",
    "    # === spaCy NER - prefer these labels but avoid duplicates ===\n",
    "    if USE_SPACY:\n",
    "        doc = nlp(text)\n",
    "        for ent in doc.ents:\n",
    "            label = ent.label_\n",
    "            if label in ('GPE', 'LOC', 'ORG'):\n",
    "                et = 'Location'\n",
    "            elif label in ('DATE',):\n",
    "                et = 'Date'\n",
    "            elif label in ('TIME',):\n",
    "                et = 'Time'\n",
    "            elif label in ('MONEY',):\n",
    "                et = 'Budget'\n",
    "            else:\n",
    "                et = label\n",
    "            start, end = ent.start_char, ent.end_char - 1\n",
    "            if any(existing['startPos'] <= start <= existing['endPos'] or existing['startPos'] <= end <= existing['endPos'] for existing in entities):\n",
    "                continue\n",
    "            # === filter blocklisted spaCy label values ===\n",
    "            if et in BLOCKLIST:\n",
    "                continue\n",
    "            entities.append({'entity': et, 'startPos': start, 'endPos': end, 'text': ent.text})\n",
    "\n",
    "    # === airport code heuristic with context ===\n",
    "    for m in re.finditer(r\"\\b([A-Z]{3})\\b\", text):\n",
    "        ctx = text[max(0, m.start()-20):m.end()+20].lower()\n",
    "        if 'airport' in ctx or 'from' in ctx or 'to' in ctx or 'arriv' in ctx or 'depart' in ctx:\n",
    "            start, end = m.start(), m.end() - 1\n",
    "            if not any(s['startPos'] <= start <= s['endPos'] for s in entities):\n",
    "                entities.append({'entity': 'AirportCode', 'startPos': start, 'endPos': end, 'text': m.group(1)})\n",
    "\n",
    "    # === normalize a few labels ===\n",
    "    for e in entities:\n",
    "        lab = e['entity']\n",
    "        if lab.lower() in ('location', 'gpe', 'loc'):\n",
    "            e['entity'] = 'Location'\n",
    "    return entities\n",
    "\n",
    "# === Load dataset robustly (file may be a dict with 'conversations' or a list of conversations) ===\n",
    "with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "conversations = dataset.get('conversations') if isinstance(dataset, dict) and 'conversations' in dataset else dataset\n",
    "\n",
    "# === Initialize LUIS structure ===\n",
    "luis_data = {\n",
    "    'luis_schema_version': '7.0.0',\n",
    "    'versionId': '0.1',\n",
    "    'name': 'FlightBookingBot',\n",
    "    'desc': 'LUIS model generated from frames dataset (improved NER)',\n",
    "    'culture': 'en-us',\n",
    "    'intents': [],\n",
    "    'entities': [],\n",
    "    'utterances': []\n",
    "}\n",
    "intents_set, entities_set = set(), set()\n",
    "\n",
    "# === Iterate conversations and turns ===\n",
    "for convo in conversations:\n",
    "    turns = convo.get('turns') if isinstance(convo, dict) else []\n",
    "    for turn in turns:\n",
    "        speaker = (turn.get('speaker') or turn.get('author') or '').lower()\n",
    "        if speaker != 'user':\n",
    "            continue\n",
    "        text = (turn.get('text') or '').strip()\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        # === Determine intent (default BookFlight) using frames/actions when available ===\n",
    "        intent = 'BookFlight'\n",
    "        for fr in turn.get('frames', []):\n",
    "            for act in fr.get('actions', []):\n",
    "                act_type = (act.get('act') or '').lower()\n",
    "                if act_type in INTENT_MAP:\n",
    "                    intent = INTENT_MAP[act_type]\n",
    "                    break\n",
    "            if intent != 'BookFlight':\n",
    "                break\n",
    "\n",
    "        intents_set.add(intent)\n",
    "\n",
    "        # === Extract Entitie ===\n",
    "        utter_entities = []\n",
    "\n",
    "        # === prefer authoritative frame values when available ===\n",
    "        for fr in turn.get('frames', []):\n",
    "            candidates = []\n",
    "            if isinstance(fr.get('info'), list):\n",
    "                candidates = fr.get('info')\n",
    "            elif isinstance(fr.get('slots'), list):\n",
    "                candidates = fr.get('slots')\n",
    "            elif isinstance(fr.get('attributes'), list):\n",
    "                candidates = fr.get('attributes')\n",
    "            for info in candidates:\n",
    "                slot = info.get('slot') or info.get('name') or info.get('key') or info.get('label')\n",
    "                value = info.get('value') or info.get('text') or info.get('values') or info.get('valueText')\n",
    "                if isinstance(value, list) and len(value) > 0:\n",
    "                    value = value[0]\n",
    "                if not slot or value is None:\n",
    "                    continue\n",
    "                normalized = ENTITY_MAP.get(slot.lower(), slot)\n",
    "                entities_set.add(normalized)\n",
    "                pos = find_positions(text, value)\n",
    "                if pos:\n",
    "                    start, end = pos\n",
    "                else:\n",
    "                    start, end = 0, 0\n",
    "                utter_entities.append({'entity': normalized, 'startPos': start, 'endPos': end, 'text': str(value)})\n",
    "        \n",
    "        # === supplement with text-based extraction ===\n",
    "        extracted = extract_entities_from_text(text)\n",
    "        for e in extracted:\n",
    "            label = e['entity']\n",
    "            entities_set.add(label)\n",
    "            if not any(u.get('startPos') == e['startPos'] and u.get('endPos') == e['endPos'] for u in utter_entities):\n",
    "                utter_entities.append({'entity': label, 'startPos': e['startPos'], 'endPos': e['endPos'], 'text': e.get('text')})\n",
    "        luis_data['utterances'].append({'text': text, 'intent': intent, 'entities': utter_entities})\n",
    "\n",
    "# === Add unique intents and entities ===\n",
    "luis_data['intents'] = [{'name': i} for i in sorted(intents_set)]\n",
    "\n",
    "# === remove blocked items from entities_set before finalizing ===\n",
    "entities_set = {e for e in entities_set if e not in BLOCKLIST}\n",
    "\n",
    "luis_data['entities'] = [{'name': e} for e in sorted(entities_set)]\n",
    "\n",
    "# === Save output (LUIS JSON) ===\n",
    "Path(OUTPUT_FILE).parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(luis_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"‚úÖ LUIS JSON created successfully!\")\n",
    "print(f\"üìÑ Saved to: {OUTPUT_FILE}\")\n",
    "print(f'üß† Intents: {len(intents_set)}, Entities: {len(entities_set)}, Utterances: {len(luis_data['utterances'])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b99091b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"luis_schema_version\": \"7.0.0\",\n",
      "\"versionId\": \"0.1\",\n",
      "\"name\": \"FlightBookingBot\",\n",
      "\"desc\": \"LUIS model generated from frames dataset (improved NER)\",\n",
      "\"culture\": \"en-us\",\n",
      "\"intents\": [\n",
      "{\n",
      "\"name\": \"BookFlight\"\n",
      "}\n",
      "],\n",
      "\"entities\": [\n",
      "{\n",
      "\"name\": \"AirportCode\"\n",
      "},\n",
      "{\n",
      "\"name\": \"Budget\"\n",
      "},\n",
      "{\n",
      "\"name\": \"CARDINAL\"\n",
      "},\n",
      "{\n",
      "\"name\": \"Date\"\n",
      "},\n",
      "{\n",
      "\"name\": \"LANGUAGE\"\n",
      "},\n",
      "{\n",
      "\"name\": \"Location\"\n",
      "},\n",
      "{\n",
      "\"name\": \"NumPassengers\"\n",
      "},\n",
      "{\n",
      "\"name\": \"ORDINAL\"\n",
      "},\n",
      "{\n",
      "\"name\": \"QUANTITY\"\n",
      "},\n",
      "{\n",
      "\"name\": \"Time\"\n",
      "}\n",
      "],\n",
      "\"utterances\": [\n",
      "{\n",
      "\"text\": \"I'd like to book a trip to Atlantis from Caprica on Saturday, August 13, 2016 for 8 adults. I have a tight budget of 1700.\",\n",
      "\"intent\": \"BookFlight\",\n",
      "\"entities\": [\n",
      "{\n",
      "\"entity\": \"Budget\",\n",
      "\"startPos\": 117,\n",
      "\"endPos\": 120,\n",
      "\"text\": \"1700\"\n",
      "},\n",
      "{\n",
      "\"entity\": \"Date\",\n",
      "\"startPos\": 52,\n",
      "\"endPos\": 76,\n",
      "\"text\": \"Saturday, August 13, 2016\"\n",
      "},\n",
      "{\n",
      "\"entity\": \"CARDINAL\",\n",
      "\"startPos\": 82,\n",
      "\"endPos\": 82,\n",
      "\"text\": \"8\"\n",
      "}\n",
      "]\n",
      "},\n",
      "{\n",
      "\"text\": \"Yes, how about going to Neverland from Caprica on August 13, 2016 for 5 adults. For this trip, my budget would be 1900.\",\n",
      "\"intent\": \"BookFlight\",\n",
      "\"entities\": [\n",
      "{\n",
      "\"entity\": \"Budget\",\n",
      "\"startPos\": 114,\n",
      "\"endPos\": 117,\n",
      "\"text\": \"1900\"\n",
      "},\n",
      "{\n",
      "\"entity\": \"Location\",\n",
      "\"startPos\": 39,\n",
      "\"endPos\": 45,\n",
      "\"text\": \"Caprica\"\n",
      "},\n",
      "{\n",
      "\"entity\": \"Date\",\n",
      "\"startPos\": 50,\n",
      "\"endPos\": 64,\n",
      "\"text\": \"August 13, 2016\"\n",
      "},\n",
      "{\n",
      "\"entity\": \"CARDINAL\",\n",
      "\"startPos\": 70,\n",
      "\"endPos\": 70,\n",
      "\"text\": \"5\"\n",
      "}\n",
      "]\n",
      "},\n",
      "{\n",
      "\"text\": \"I have no flexibility for dates... but I can leave from Atlantis rather than Caprica. How about that?\",\n",
      "\"intent\": \"BookFlight\",\n",
      "\"entities\": []\n",
      "},\n",
      "{\n",
      "\"text\": \"I suppose I'll speak with my husband to see if we can choose other dates, and then I'll come back to you.Thanks for your help\",\n",
      "\"intent\": \"BookFlight\",\n",
      "\"entities\": []\n",
      "},\n",
      "{\n",
      "\"text\": \"Hello, I am looking to book a vacation from Gotham City to Mos Eisley for $2100.\",\n",
      "\"intent\": \"BookFlight\",\n",
      "\"entities\": [\n",
      "{\n",
      "\"entity\": \"Budget\",\n",
      "\"startPos\": 75,\n",
      "\"endPos\": 78,\n",
      "\"text\": \"2100\"\n",
      "},\n",
      "{\n",
      "\"entity\": \"Location\",\n",
      "\"startPos\": 44,\n",
      "\"endPos\": 54,\n",
      "\"text\": \"Gotham City\"\n",
      "}\n",
      "]\n",
      "},\n",
      "{\n",
      "\"text\": \"What about a trip from Gotham City to Neverland for the same budget?\",\n",
      "\"intent\": \"BookFlight\",\n",
      "\"entities\": [\n",
      "{\n",
      "\"entity\": \"Location\",\n",
      "\"startPos\": 23,\n",
      "\"endPos\": 33,\n",
      "\"text\": \"Gotham City\"\n",
      "}\n",
      "]\n",
      "},\n",
      "{\n",
      "\"text\": \"Would any packages to Mos Eisley be available if I increase my budget to $2500?\",\n",
      "\"intent\": \"BookFlight\",\n",
      "\"entities\": [\n",
      "{\n",
      "\"entity\": \"Budget\",\n",
      "\"startPos\": 74,\n",
      "\"endPos\": 77,\n",
      "\"text\": \"2500\"\n",
      "}\n",
      "]\n",
      "},\n",
      "{\n",
      "\"text\": \"You know what, I'd like to try and visit Neverland\",\n",
      "\"intent\": \"BookFlight\",\n",
      "\"entities\": []\n",
      "},\n",
      "{\n",
      "\"text\": \"Do you have any trips from Gotham City to Kobe for my original budget of $2100?\",\n",
      "\"intent\": \"BookFlight\",\n",
      "\"entities\": [\n",
      "{\n",
      "\"entity\": \"Budget\",\n",
      "\"startPos\": 74,\n",
      "\"endPos\": 77,\n",
      "\"text\": \"2100\"\n",
      "},\n",
      "{\n",
      "\"entity\": \"Location\",\n",
      "\"startPos\": 27,\n",
      "\"endPos\": 37,\n",
      "\"text\": \"Gotham City\"\n",
      "},\n",
      "{\n",
      "\"entity\": \"Location\",\n",
      "\"startPos\": 42,\n",
      "\"endPos\": 45,\n",
      "\"text\": \"Kobe\"\n",
      "}\n",
      "]\n",
      "},\n",
      "{\n",
      "\"text\": \"No, that's too far for me. I need a flight that leaves from Birmingham.\",\n",
      "\"intent\": \"BookFlight\",\n",
      "\"entities\": [\n",
      "{\n",
      "\"entity\": \"Location\",\n",
      "\"startPos\": 60,\n",
      "\"endPos\": 69,\n",
      "\"text\": \"Birmingham\"\n",
      "}\n",
      "]\n",
      "},\n",
      "{\n",
      "\"text\": \"How many days would I be in Kobe?\",\n",
      "\"intent\": \"BookFlight\",\n",
      "\"entities\": [\n",
      "{\n",
      "\"entity\": \"Location\",\n",
      "\"startPos\": 28,\n",
      "\"endPos\": 31,\n",
      "\"text\": \"Kobe\"\n",
      "}\n",
      "]\n",
      "},\n",
      "{\n",
      "\"text\": \"What would the price be if I shortened my trip by one day?\",\n",
      "\"intent\": \"BookFlight\",\n",
      "\"entities\": [\n",
      "{\n",
      "\"entity\": \"Date\",\n",
      "\"startPos\": 50,\n",
      "\"endPos\": 56,\n",
      "\"text\": \"one day\"\n",
      "}\n",
      "]\n",
      "},\n",
      "{\n",
      "\"text\": \"Ok, then I would like to purchase this package. What activities are included in this package?\",\n",
      "\"intent\": \"BookFlight\",\n",
      "\"entities\": []\n",
      "},\n",
      "{\n",
      "\"text\": \"Yes, I would like to book this package.\",\n",
      "\"intent\": \"BookFlight\",\n",
      "\"entities\": []\n",
      "},\n",
      "{\n",
      "\"text\": \"Hello there i am looking to go on a vacation with my family to Gotham City, can you help me?\",\n",
      "\"intent\": \"BookFlight\",\n",
      "\"entities\": [\n",
      "{\n",
      "\"entity\": \"Location\",\n",
      "\"startPos\": 63,\n",
      "\"endPos\": 73,\n",
      "\"text\": \"Gotham City\"\n",
      "}\n",
      "]\n",
      "},\n",
      "{\n",
      "\"text\": \"Not sure when we want to leave, but we are 12 kids and 5 adults\",\n",
      "\"intent\": \"BookFlight\",\n",
      "\"entities\": [\n",
      "{\n",
      "\"entity\": \"CARDINAL\",\n",
      "\"startPos\": 43,\n",
      "\"endPos\": 44,\n",
      "\"text\": \"12\"\n",
      "},\n",
      "{\n",
      "\"entity\": \"CARDINAL\",\n",
      "\"startPos\": 55,\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load JSON file\n",
    "with open(OUTPUT_FILE, 'r') as file:\n",
    "    for i in range(250):  # Read first 150 lines\n",
    "        line = file.readline()\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1b4def",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CompViz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
