{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4efdd0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4817f70d",
   "metadata": {},
   "source": [
    "# Fly Me\n",
    "A travel provider for individuals and professionals. \n",
    "\n",
    "## Overview\n",
    "The chatbot project - Jupyter Notebook\n",
    "Fly Me a travel agency has launched an ambitious project to develop a *chatbot* to *help users choose a travel offer*.\n",
    "\n",
    "The first phase of this project is to **develop an MVP** that will help Fly Me employees to easily book airline tickets for their holidays.\n",
    "\n",
    "This first MVP will allow us to test the concept and performance of the chatbot quickly and on a large scale.\n",
    "\n",
    "As this project is iterative, we have limited the features of the chatbot V1. It must be able to identify the following five elements in the user's request:\n",
    "\n",
    "* Departure city\n",
    "* Destination city\n",
    "* Desired flight departure date\n",
    "* Desired flight return date\n",
    "* Maximum budget for the total price of tickets\n",
    "\n",
    "If one of the elements is missing, the chatbot must be able to ask the user the relevant questions in order to fully understand the request. When the chatbot thinks it has understood all the elements of the user's request, it must be able to reformulate the user's request and ask the user for confirmation.\n",
    "\n",
    "## Tools and Technologies used\n",
    "To carry out this project, we will need to use the following tools and technologies:\n",
    "\n",
    "* The Microsoft Bot Framework source code for Python “Microsoft Bot Framework SDK v4 for Python” \n",
    "* The Azure LUIS Cognitive Service which allows you to perform a semantic analysis of a message entered by the user and structure it for processing by the bot (it should allow you to identify the five elements requested)\n",
    "* The Azure Web App service that allows you to run a web application on the Azure Cloud (you won't need to use the Azure Bot service)\n",
    "* The Bot Framework Emulator which will allow you to test your chatbot locally and in production. It is an interface that allows a user to interact with the chatbot.\n",
    "\n",
    "## Data\n",
    "The data used to train the LUIS model comes from a dataset of conversations between users and travel agents. This dataset is in JSON format and contains a total of 1500 conversations. Each conversation consists of a series of messages exchanged between the user and the travel agent. The messages contain information about the user's travel preferences, such as departure city, destination city, travel dates, and budget. For more details about the dataset, please refer to the original source: [Frames Dataset](https://www.microsoft.com/en-us/research/project/frames-dataset/).\n",
    "\n",
    "For the purpose of this project, the dataset is divided into two parts: a training set and a test set. The training set contains 80% conversations and is used to train the LUIS model, the remainder of the conversations are assigned to the test set. The test set is used to evaluate the performance of the LUIS model.\n",
    "The dataset is available in the `data/frames_dataset` directory of the project. The main file is `frames.json`, which contains all the conversations. The file is structured as follows:\n",
    "```json\n",
    "{\n",
    "  \"conversations\": [\n",
    "    {\n",
    "      \"id\": \"1\",\n",
    "      \"turns\": [\n",
    "        {\n",
    "          \"speaker\": \"user\",\n",
    "          \"text\": \"I want to book a flight from New York to Paris.\"\n",
    "        },\n",
    "        {\n",
    "          \"speaker\": \"agent\",\n",
    "          \"text\": \"Sure, when do you want to depart?\"\n",
    "        },\n",
    "        ...\n",
    "      ]\n",
    "    },\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "```\n",
    "The `turns` field contains a list of messages exchanged between the user and the travel agent. Each message has a `speaker` field indicating who sent the message (either \"user\" or \"agent\") and a `text` field containing the content of the message.\n",
    "\n",
    "## Objective\n",
    "The objective of this notebook is to explore and preprocess the dataset to prepare it for training the LUIS model. This includes loading the JSON file, flattening the nested structure, and extracting relevant information from the conversations. We will use the `pandas` library to manipulate the data and perform basic exploratory data analysis (EDA) to understand the distribution of the different entities and intents in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c6b50e",
   "metadata": {},
   "source": [
    "## What is a bot?\n",
    "Bots provide an experience that feels less like using a computer and more like dealing with a person—or intelligent robot. Bots can used to shift simple, repetitive tasks—such as taking a dinner reservation or gathering profile information—onto automated systems that may no longer require direct human intervention. Users converse with a bot using text, interactive cards, and speech. A bot interaction can be a quick answer to a question or an involved conversation that intelligently provides access to services.\n",
    "\n",
    "In creating a bot, we define how the bot interacts with users, what services it connects to, and how it processes information. We can create bots that run in a variety of environments, including websites, apps, Microsoft Teams, Skype, Slack, Facebook Messenger, and more.\n",
    "For a detailed overview of bots, see [What are bots?](https://learn.microsoft.com/en-us/azure/bot-service/bot-service-overview-introduction?view=azure-bot-service-4.0). \n",
    "\n",
    "In this project we will create a bot that can help users book flights using the Language Understanding Interface System (LUIS) or the Conversational Language Understanding (CLU) model from Microsoft. To create the model we will define intents, entities, and utterances.\n",
    "\n",
    "1. **Intents**: \n",
    "\n",
    "An intent represents a task or action that the user wants to perform. It is the purpose of the user's input. For example, in our flight booking bot, we might have intents such as \"BookFlight\", \"CancelFlight\", and \"GetFlightStatus\".\n",
    "\n",
    "2. **Entities**: \n",
    "\n",
    "Entities are used to extract specific pieces of information from the user's input that are relevant to the intent. For example, in the \"BookFlight\" intent, we might have entities such as \"DepartureCity\", \"DestinationCity\", \"DepartureDate\", \"ReturnDate\", and \"Budget\".\n",
    "\n",
    "3. **Utterances**: \n",
    "\n",
    "Utterances are the actual phrases or sentences that users might say to express their intent. For example, for the \"BookFlight\" intent, some example utterances might be \"I want to book a flight from New York to Paris on June 1st and return on June 10th with a budget of $1000\" or \"Can you help me find a flight to London next month?\".\n",
    "\n",
    "In summary:\n",
    "* An **intent** represents the purpose of a user's input or a task/action a user wants to perform. It is the meaning of an utterance. In this case, the intent is to book a flight.\n",
    "* An **entity** are used to add specific context to intents. Represents a specific piece of information that is relevant to the intent. In this case, the entities are the departure city, destination city, departure date, return date, and budget.\n",
    "* An **utterance** is a phrase(s) that a user might enter when interacting with the application. It is a specific example of a user's input that expresses the intent. In this case, an utterance could be \"I want to book a flight from New York to Paris on June 1st and return on June 10th with a budget of $1000.\"\n",
    "\n",
    "We create a model by defining intents and associating them with one or more utterances. We define the intents we want the model to understand. Every model must have a **None** intent (used to explicitly identify utterances that a user might submit, but for which there is no specific action required (for example, conversational greetings like \"hello\") or that fall outside of the scope of the domain for this model). We must define the entities that are relevant to the intent and annotate the utterances with the appropriate entities. This helps the model learn to recognize the intent and extract the relevant entities from user input. \n",
    "\n",
    "The model can then recognize the intent to book a flight and extract the relevant entities from the user's input. This model can then be integrated into a chatbot application to help users book flights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90555fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened DataFrame shape: (19986, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>wizard_id</th>\n",
       "      <th>id</th>\n",
       "      <th>labels.userSurveyRating</th>\n",
       "      <th>labels.wizardSurveyTaskSuccessful</th>\n",
       "      <th>turns.text</th>\n",
       "      <th>turns.author</th>\n",
       "      <th>turns.timestamp</th>\n",
       "      <th>turns.labels.acts</th>\n",
       "      <th>turns.labels.acts_without_refs</th>\n",
       "      <th>turns.labels.active_frame</th>\n",
       "      <th>turns.labels.frames</th>\n",
       "      <th>turns.db.result</th>\n",
       "      <th>turns.db.search</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U22HTHYNP</td>\n",
       "      <td>U21DKG18C</td>\n",
       "      <td>e2c0fc6c-2134-4891-8353-ef16d8412c9a</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>I'd like to book a trip to Atlantis from Capri...</td>\n",
       "      <td>user</td>\n",
       "      <td>1.471272e+12</td>\n",
       "      <td>[{'args': [{'val': 'book', 'key': 'intent'}], ...</td>\n",
       "      <td>[{'args': [{'val': 'book', 'key': 'intent'}], ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'info': {'intent': [{'val': 'book', 'negated...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U22HTHYNP</td>\n",
       "      <td>U21DKG18C</td>\n",
       "      <td>e2c0fc6c-2134-4891-8353-ef16d8412c9a</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>Hi...I checked a few options for you, and unfo...</td>\n",
       "      <td>wizard</td>\n",
       "      <td>1.471272e+12</td>\n",
       "      <td>[{'args': [{'val': [{'annotations': [], 'frame...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'info': {'intent': [{'val': 'book', 'negated...</td>\n",
       "      <td>[[{'trip': {'returning': {'duration': {'hours'...</td>\n",
       "      <td>[{'ORIGIN_CITY': 'Porto Alegre', 'PRICE_MIN': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U22HTHYNP</td>\n",
       "      <td>U21DKG18C</td>\n",
       "      <td>e2c0fc6c-2134-4891-8353-ef16d8412c9a</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>Yes, how about going to Neverland from Caprica...</td>\n",
       "      <td>user</td>\n",
       "      <td>1.471273e+12</td>\n",
       "      <td>[{'args': [{'val': 'Neverland', 'key': 'dst_ci...</td>\n",
       "      <td>[{'args': [{'val': 'Neverland', 'key': 'dst_ci...</td>\n",
       "      <td>2</td>\n",
       "      <td>[{'info': {'intent': [{'val': 'book', 'negated...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id  wizard_id                                    id  \\\n",
       "0  U22HTHYNP  U21DKG18C  e2c0fc6c-2134-4891-8353-ef16d8412c9a   \n",
       "1  U22HTHYNP  U21DKG18C  e2c0fc6c-2134-4891-8353-ef16d8412c9a   \n",
       "2  U22HTHYNP  U21DKG18C  e2c0fc6c-2134-4891-8353-ef16d8412c9a   \n",
       "\n",
       "   labels.userSurveyRating  labels.wizardSurveyTaskSuccessful  \\\n",
       "0                      4.0                               True   \n",
       "1                      4.0                               True   \n",
       "2                      4.0                               True   \n",
       "\n",
       "                                          turns.text turns.author  \\\n",
       "0  I'd like to book a trip to Atlantis from Capri...         user   \n",
       "1  Hi...I checked a few options for you, and unfo...       wizard   \n",
       "2  Yes, how about going to Neverland from Caprica...         user   \n",
       "\n",
       "   turns.timestamp                                  turns.labels.acts  \\\n",
       "0     1.471272e+12  [{'args': [{'val': 'book', 'key': 'intent'}], ...   \n",
       "1     1.471272e+12  [{'args': [{'val': [{'annotations': [], 'frame...   \n",
       "2     1.471273e+12  [{'args': [{'val': 'Neverland', 'key': 'dst_ci...   \n",
       "\n",
       "                      turns.labels.acts_without_refs  \\\n",
       "0  [{'args': [{'val': 'book', 'key': 'intent'}], ...   \n",
       "1                                                NaN   \n",
       "2  [{'args': [{'val': 'Neverland', 'key': 'dst_ci...   \n",
       "\n",
       "   turns.labels.active_frame  \\\n",
       "0                          1   \n",
       "1                          1   \n",
       "2                          2   \n",
       "\n",
       "                                 turns.labels.frames  \\\n",
       "0  [{'info': {'intent': [{'val': 'book', 'negated...   \n",
       "1  [{'info': {'intent': [{'val': 'book', 'negated...   \n",
       "2  [{'info': {'intent': [{'val': 'book', 'negated...   \n",
       "\n",
       "                                     turns.db.result  \\\n",
       "0                                                NaN   \n",
       "1  [[{'trip': {'returning': {'duration': {'hours'...   \n",
       "2                                                NaN   \n",
       "\n",
       "                                     turns.db.search  \n",
       "0                                                NaN  \n",
       "1  [{'ORIGIN_CITY': 'Porto Alegre', 'PRICE_MIN': ...  \n",
       "2                                                NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Flatten nested JSON columns into a flat DataFrame\n",
    "from pandas import json_normalize\n",
    "# Load JSON file\n",
    "path = '../data/frames_dataset/frames.json'\n",
    "with open(path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# create a flat DataFrame from the loaded `data` variable\n",
    "try:\n",
    "    # Normalize `data` to have all dicts/lists at the same level\n",
    "    if 'data' in globals() and (isinstance(data, list) or isinstance(data, dict)):\n",
    "        df_flat = json_normalize(data, sep='.')\n",
    "    else:\n",
    "        # Fall back to normalizing rows from the existing DataFrame `df`\n",
    "        df_flat = json_normalize(df.to_dict(orient='records'), sep='.')\n",
    "except Exception as e:\n",
    "    print('Normalization error:', e)\n",
    "    df_flat = df.copy()\n",
    "\n",
    "# Handle columns that are lists of dicts: detect and explode+normalize them\n",
    "for col in df_flat.columns.tolist():\n",
    "    # If the column contains lists of dicts, expand them\n",
    "    if df_flat[col].apply(lambda x: isinstance(x, list) and len(x) > 0 and isinstance(x[0], dict)).any():\n",
    "        # Explode the list so each element becomes its own row, then normalize that column\n",
    "        exploded = df_flat.explode(col).reset_index(drop=True)\n",
    "        # Normalize the exploded column (it may contain dicts or NaN)\n",
    "        expanded = json_normalize(exploded[col].dropna().tolist(), sep='.')\n",
    "        # Prefix new columns with the original column name\n",
    "        expanded = expanded.add_prefix(col + '.')\n",
    "        # Join back to exploded frame (align by index)\n",
    "        exploded = exploded.drop(columns=[col]).join(expanded)\n",
    "        df_flat = exploded\n",
    "\n",
    "# Show results\n",
    "print('Flattened DataFrame shape:', df_flat.shape)\n",
    "display(df_flat.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fbf2df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"I'd like to book a trip to Atlantis from Caprica on Saturday, August 13, 2016 for 8 adults. I have a tight budget of 1700.\",\n",
       "       'Hi...I checked a few options for you, and unfortunately, we do not currently have any trips that meet this criteria.  Would you like to book an alternate travel option?',\n",
       "       'Yes, how about going to Neverland from Caprica on August 13, 2016 for 5 adults. For this trip, my budget would be 1900.',\n",
       "       ..., 'Consider it done! Have a good trip :slightly_smiling_face:',\n",
       "       'Thanks!', 'My pleasure!'], shape=(19986,), dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flat['turns.text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "879b98be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64bit\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(platform.architecture()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa8d399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = df_flat.sample(5, random_state=42)\n",
    "# json_string = sample.to_json()\n",
    "# print(json_string)\n",
    "# # Save the flattened DataFrame to a CSV file\n",
    "# sample.to_json('../data/frames_dataset/sample_frames.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b97de2",
   "metadata": {},
   "source": [
    "## Create Conversation Analysis Client\n",
    "To create a Conversation Analysis client, you need to install the `azure-ai-conversationanalysis` package. You can do this using pip:\n",
    "\n",
    "```bashpip install azure-ai-conversationanalysis\n",
    "```\n",
    "```## Import necessary libraries\n",
    "```python\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "# Load the JSON file\n",
    "with open('../data/frames_dataset/frames.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "# Normalize the JSON data to flatten the structure\n",
    "df = json_normalize(data['conversations'], 'turns', ['id'])\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475fec50",
   "metadata": {},
   "source": [
    "#### Generate a LUIS JSON from the frames.json file\n",
    "This dataset is in JSON format and contains a total of 1500 conversations. Each conversation consists of a series of messages exchanged between the user and the travel agent. The messages contain information about the user's travel preferences, such as departure city, destination city, travel dates, and budget.\n",
    "The dataset is divided into two parts: a training set and a test set. The training set contains 1200 conversations and is used to train the LUIS model. The test set contains 300 conversations and is used to evaluate the performance of the LUIS model.\n",
    "The dataset is available in the `data/frames_dataset` directory of the project. The main file is `frames.json`, which contains all the conversations. The file is structured as follows:\n",
    "```json\n",
    "{\n",
    "  \"conversations\": [\n",
    "    {\n",
    "      \"id\": \"1\",\n",
    "      \"turns\": [\n",
    "        {\n",
    "          \"speaker\": \"user\",\n",
    "          \"text\": \"I want to book a flight from New York to Paris.\"\n",
    "        },\n",
    "        {\n",
    "          \"speaker\": \"agent\",\n",
    "          \"text\": \"Sure, when do you want to depart?\"\n",
    "        },\n",
    "        ...\n",
    "      ]\n",
    "    },\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "```\n",
    "The dataset contains conversation in French and English. The LUIS model must be able to understand both languages.\n",
    "Each conversation is identified by a unique ID and consists of multiple turns. Each turn has a speaker (either \"user\" or \"agent\") and the text of the message.\n",
    "The structure:\n",
    "* multiple turns per conversation (user and agent)\n",
    "* each turn has a speaker and text (frames)\n",
    "* information about travel preferences (departure city, destination city, travel dates, budget)\n",
    "The goal is to extract the relevant information (<user utterances>) from the conversations and format it according to the LUIS JSON structure. This includes identifying the intents and entities in the user's messages and structuring (map slots/entities) them in a way that LUIS can understand.\n",
    "\n",
    "**LUIS JSON Structure:**\n",
    "```json\n",
    "{\n",
    "  \"luis_schema_version\": \"7.0.0\",\n",
    "  \"versionId\": \"0.1\",\n",
    "  \"name\": \"TravelBooking\",\n",
    "  \"desc\": \"LUIS model for travel booking chatbot\",\n",
    "  \"culture\": \"en-us\",\n",
    "  \"intents\": [\n",
    "    {\n",
    "      \"name\": \"BookFlight\"\n",
    "    }\n",
    "  ],\n",
    "  \"entities\": [\n",
    "    {\n",
    "      \"name\": \"DepartureCity\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"DestinationCity\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"DepartureDate\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"ReturnDate\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Budget\"\n",
    "    }\n",
    "  ],\n",
    "  \"composites\": [],\n",
    "  \"closedLists\": [],\n",
    "  \"patternAnyEntities\": [],\n",
    "  \"regex_entities\": [],\n",
    "  \"prebuiltEntities\": [],\n",
    "  \"model_features\": [],\n",
    "  \"regex_features\": [],\n",
    "  \"utterances\": [\n",
    "    {\n",
    "      \"text\": \"I want to book a flight from New York to Paris.\",\n",
    "      \"intent\": \"BookFlight\",\n",
    "      \"entities\": [\n",
    "        {\n",
    "          \"entity\": \"DepartureCity\",\n",
    "          \"startPos\": 27,\n",
    "          \"endPos\": 34\n",
    "        },\n",
    "        {\n",
    "          \"entity\": \"DestinationCity\",\n",
    "          \"startPos\": 38,\n",
    "          \"endPos\": 42\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"patterns\": []\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1eee0792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Date-first extraction using dateparser (prefer these spans) ===\n",
    "# Improved date and budget entity extraction using dateparser + heuristics\n",
    "import dateparser\n",
    "import re\n",
    "\n",
    "MONTH_WORDS = r'\\b(?:jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec|january|february|march|april|may|june|july|august|september|october|november|december)\\b'\n",
    "\n",
    "\n",
    "# === date spans extraction ===\n",
    "def find_date_spans(text):\n",
    "    \"\"\"\n",
    "    Return list of (start, end, matched_text, parsed_dt) found by dateparser.search.search_dates.\n",
    "    Fall back to a few explicit regexes (ISO) if needed.\n",
    "    \"\"\"\n",
    "    spans = []\n",
    "    # === try dateparser search first ===\n",
    "    if dateparser is not None:\n",
    "        try:\n",
    "            res = dateparser.search.search_dates(text, settings={'PREFER_DATES_FROM': 'future', 'RETURN_AS_TIMEZONE_AWARE': False})\n",
    "        except Exception:\n",
    "            res = None\n",
    "        if res:\n",
    "            for match_text, dt in res:\n",
    "                # === find first occurrence of match_text (case-insensitive) and use it ===\n",
    "                m = re.search(re.escape(match_text), text, flags=re.IGNORECASE)\n",
    "                if m:\n",
    "                    spans.append((m.start(), m.end()-1, match_text, dt))\n",
    "    # === fallback: explicit ISO-ish regex spans not caught by dateparser ===\n",
    "    for m in re.finditer(r'\\b20\\d{2}[/-]\\d{1,2}[/-]\\d{1,2}\\b', text):\n",
    "        spans.append((m.start(), m.end()-1, text[m.start():m.end()], None))\n",
    "    return spans\n",
    "\n",
    "# === Improved numeric-as-budget decision (use date spans + context cues + year heuristics) ===\n",
    "CURRENCY_CUES = ['$', 'usd', 'dollars', 'eur', '€', '£', 'budget', 'price', 'cost', 'fare', 'ticket', 'pay']\n",
    "\n",
    "def is_token_overlapping_spans(start, end, spans):\n",
    "    return any(s <= start <= e or s <= end <= e for (s,e, *_ ) in spans)\n",
    "\n",
    "def is_likely_budget_token(text, start, end, date_spans):\n",
    "    \"\"\"\n",
    "    Return True if numeric token at (start,end) is likely a budget, False otherwise.\n",
    "    Uses date_spans to avoid classifying date tokens.\n",
    "    \"\"\"\n",
    "    # don't label if overlaps an already-detected date\n",
    "    if is_token_overlapping_spans(start, end, date_spans):\n",
    "        return False\n",
    "\n",
    "    window = text[max(0, start-30):min(len(text), end+30)].lower()\n",
    "\n",
    "    # strong currency cues -> budget\n",
    "    if any(cue in window for cue in CURRENCY_CUES):\n",
    "        return True\n",
    "    # currency symbol immediately before e.g. $1900\n",
    "    if start > 0 and text[start-1] in ['$', '€', '£']:\n",
    "        return True\n",
    "\n",
    "    token = text[start:end+1]\n",
    "    # numeric-only value\n",
    "    try:\n",
    "        val = int(re.sub(r'[^0-9]', '', token))\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "    # heuristic rules for years vs price:\n",
    "    # if token is a 4-digit year within typical year range and there's a month word nearby -> treat as date\n",
    "    if len(token) == 4 and 1900 <= val <= 2035:\n",
    "        month_nearby = re.search(MONTH_WORDS, window, flags=re.IGNORECASE)\n",
    "        if month_nearby:\n",
    "            return False  # likely a year in date context\n",
    "        # if sentence contains explicit date tokens (24, Aug etc) treat as Date\n",
    "        if re.search(r'\\b\\d{1,2}\\b', window) and re.search(MONTH_WORDS, window, flags=re.IGNORECASE):\n",
    "            return False\n",
    "\n",
    "    # numeric-value heuristics: consider values >=100 as plausible budgets (tunable)\n",
    "    if val >= 100 and val <= 1000000:\n",
    "        # If text contains words like 'on' before token plus month words, that might be a date: be conservative\n",
    "        before = text[max(0, start-10):start].lower()\n",
    "        if re.search(r'\\bon\\b', before) and re.search(MONTH_WORDS, text, flags=re.IGNORECASE):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6d593ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ sklearn available but no persisted classifier found; skipping training as requested\n",
      "✅ Wrote 9562 utterances to: ../data/frames_dataset/luis_flight_booking.json (removed 845 duplicates)\n",
      "✅ LUIS JSON created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Using spaCy NER extraction of LUIS utterances from frames.json\n",
    "# - Uses spaCy NER when available (fallback to rule-based regex and heuristics)\n",
    "# - Adds more entity patterns (airport codes, times, currencies, passenger counts)\n",
    "\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# === Configuration, Reuse existing notebook constants if available, else fall back ===\n",
    "INPUT_FILE = globals().get('INPUT_FILE', '../data/frames_dataset/frames.json')\n",
    "OUTPUT_FILE = globals().get('OUTPUT_FILE', '../data/frames_dataset/luis_flight_booking.json')\n",
    "\n",
    "# === Intent mapping from frame actions ===\n",
    "INTENT_MAP = globals().get('INTENT_MAP', {\n",
    "    'book': 'BookFlight',\n",
    "    'inform': 'ProvideInfo',\n",
    "    'offer': 'OfferFlight',\n",
    "    'request': 'RequestInfo',\n",
    "    'confirm': 'ConfirmBooking',\n",
    "    'greet': 'Greet',\n",
    "    'thankyou': 'ThankYou',\n",
    "    'select': 'SelectOption',\n",
    "    'deny': 'DenyRequest',\n",
    "    'ack': 'Acknowledge'\n",
    "})\n",
    "\n",
    "# === IATA airport code mapping (3-letter codes) ===\n",
    "# This is a small sample; in production, should use a comprehensive list or API lookup as needed ===\n",
    "IATA_MAP = {\n",
    "    \"LON\": \"London\",\n",
    "    \"NYC\": \"New York\",\n",
    "    \"SFO\": \"San Francisco\",\n",
    "    \"SEA\": \"Seattle\",\n",
    "    \"CHI\": \"Chicago\",\n",
    "    \"BOS\": \"Boston\",\n",
    "    \"ATL\": \"Atlanta\",\n",
    "    \"DFW\": \"Dallas\",\n",
    "    \"DEN\": \"Denver\",\n",
    "    \"MIA\": \"Miami\",\n",
    "    \"LAX\": \"Los Angeles\",\n",
    "    \"PAR\": \"Paris\",\n",
    "    \"BER\": \"Berlin\",\n",
    "    \"ROM\": \"Rome\",\n",
    "    \"AMS\": \"Amsterdam\",\n",
    "    \"BKK\": \"Bangkok\",\n",
    "    \"HKG\": \"Hong Kong\",\n",
    "    \"DEL\": \"Delhi\",\n",
    "    \"DXB\": \"Dubai\",\n",
    "    \"SYD\": \"Sydney\"\n",
    "}\n",
    "\n",
    "# === Helper to position finder for IATA codes ===\n",
    "def map_iata_to_city(value):\n",
    "    if not value:\n",
    "        return value\n",
    "    val = str(value).strip().upper()\n",
    "    return IATA_MAP.get(val, value)\n",
    "\n",
    "# === Helper to find positions of value in text (case-insensitive) ===\n",
    "def find_positions(text, value):\n",
    "    if not value or not text:\n",
    "        return None\n",
    "    s = str(value).strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    m = re.search(re.escape(s), text, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        return m.start(), m.end() - 1\n",
    "    norm_text = re.sub(r'\\s+', ' ', text).lower()\n",
    "    norm_value = re.sub(r'\\s+', ' ', s).lower()\n",
    "    idx = norm_text.find(norm_value)\n",
    "    if idx != -1:\n",
    "        words = norm_value.split()\n",
    "        pattern = r'\\\\b' + r'\\\\s+'.join(map(re.escape, words)) + r'\\\\b'\n",
    "        m2 = re.search(pattern, text, flags=re.IGNORECASE)\n",
    "        if m2:\n",
    "            return m2.start(), m2.end() - 1\n",
    "    mapped = map_iata_to_city(s)\n",
    "    if mapped and mapped.lower() != norm_value:\n",
    "        norm_mapped = re.sub(r'\\s+', ' ', mapped).lower()\n",
    "        idx2 = norm_text.find(norm_mapped)\n",
    "        if idx2 != -1:\n",
    "            words = norm_mapped.split()\n",
    "            pattern = r'\\\\b' + r'\\\\s+'.join(map(re.escape, words)) + r'\\\\b'\n",
    "            m2 = re.search(pattern, text, flags=re.IGNORECASE)\n",
    "            if m2:\n",
    "                return m2.start(), m2.end() - 1\n",
    "    from difflib import get_close_matches\n",
    "    words = norm_text.split()\n",
    "    cm = get_close_matches(norm_value, words, n=1, cutoff=0.8)\n",
    "    if cm:\n",
    "        word = cm[0]\n",
    "        m3 = re.search(re.escape(word), text, flags=re.IGNORECASE)\n",
    "        if m3:\n",
    "            return m3.start(), m3.end() - 1\n",
    "    return None\n",
    "\n",
    "# === Entity normalization mapping ===\n",
    "# ENTITY_MAP = globals().get('ENTITY_MAP', {})\n",
    "\n",
    "# === Blocklist certain spaCy labels and unwanted entity names ===\n",
    "BLOCKLIST =  globals().get('BLOCKLIST', {'EVENT', 'FAC', 'LAW', 'PRODUCT', 'NORP', 'PERCENT', 'PERSON', 'WORK_OF_ART'})\n",
    "# add common unwanted entity names\n",
    "BLOCKLIST.update({'Act', 'Action', 'Agent', 'Airline', 'Airlines', 'Class', 'Classes', 'Confirmation', 'Email'\n",
    "                 , 'Emails', 'Flight', 'Flights', 'Meal', 'Meals', 'Name', 'Names'\n",
    "                 , 'Preference', 'Preferences', 'Seat', 'Seats', 'Status', 'Statuses', 'Ticket', 'Tickets'\n",
    "                 , 'Type', 'Types', 'Value', 'Values'})\n",
    "\n",
    "# === entity patterns for regex-based extraction ===\n",
    "ENTITY_PATTERNS = [\n",
    "    (\"DepartureDate\", re.compile(r\"\\b(20\\d{2}[\\/-]\\d{1,2}[\\/-]\\d{1,2})\\b\")),\n",
    "    (\"DepartureDate\", re.compile(r\"\\b(\\d{1,2}\\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Sept|Oct|Nov|Dec)[a-z]*\\s+20\\d{2})\\b\", re.IGNORECASE)),\n",
    "    (\"Time\", re.compile(r\"\\b(\\d{1,2}:(?:\\d{2})(?:\\s?[APMapm]{2})?)\\b\")),\n",
    "    (\"Budget\", re.compile(r\"\\b\\$\\s?\\d{1,3}(?:,\\d{3})*(?:\\.\\d{1,2})?\\b\")),\n",
    "    (\"Budget\", re.compile(r\"\\b(?:usd|dollars|eur|€|£)\\s?\\d{1,3}(?:,\\d{3})*(?:\\.\\d{1,2})?\\b\", re.IGNORECASE)),\n",
    "    (\"AirportCode\", re.compile(r\"\\b[A-Z]{3}\\b\")),\n",
    "    (\"NumPassengers\", re.compile(r\"\\b(\\d+)\\s*(?:passengers|people|persons|pax)\\b\", re.IGNORECASE)),\n",
    "]\n",
    "\n",
    "# === Try to enable spaCy (dependency parsing + NER) and sklearn for optional classifier ===\n",
    "USE_SPACY = False\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    USE_SPACY = True\n",
    "except Exception:\n",
    "    USE_SPACY = False\n",
    "\n",
    "# === Try to import sklearn for optional intent classifier ===\n",
    "HAVE_SKLEARN = False\n",
    "clf = None\n",
    "vectorizer = None\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from collections import Counter\n",
    "    import joblib, os\n",
    "    HAVE_SKLEARN = True\n",
    "except Exception:\n",
    "    HAVE_SKLEARN = False\n",
    "\n",
    "# If True, force training and persisting the intent classifier even if training counts are small\n",
    "FORCE_TRAIN = True\n",
    "\n",
    "# === Helper for entity extraction ===\n",
    "def extract_entities_from_text(text):\n",
    "    date_spans = find_date_spans(text)\n",
    "    entities = []\n",
    "    for s, e, match, dt in date_spans:\n",
    "        entities.append({'category': 'Date', 'offset': s, 'endPos': e, 'text': match})\n",
    "    for etype, pattern in ENTITY_PATTERNS:\n",
    "        try:\n",
    "            for m in pattern.finditer(text):\n",
    "                s, e = m.start(), m.end()-1\n",
    "                if is_token_overlapping_spans(s, e, [(ds, de) for ds, de, _, _ in date_spans]):\n",
    "                    continue\n",
    "                if any(ent['offset'] <= s <= ent['endPos'] or ent['offset'] <= e <= ent['endPos'] for ent in entities):\n",
    "                    continue\n",
    "                entities.append({'category': etype, 'offset': s, 'endPos': e, 'text': text[s:e+1]})\n",
    "        except re.error:\n",
    "            continue\n",
    "    for m in re.finditer(r\"\\b\\d{3,6}\\b\", text):\n",
    "        s, e = m.start(), m.end()-1\n",
    "        if is_token_overlapping_spans(s, e, [(ds, de) for ds, de, _, _ in date_spans]):\n",
    "            continue\n",
    "        if any(ent['offset'] <= s <= ent['endPos'] or ent['offset'] <= e <= ent['endPos'] for ent in entities):\n",
    "            continue\n",
    "        if is_likely_budget_token(text, s, e, date_spans):\n",
    "            span_text = text[s:e+1]\n",
    "            entities.append({'category': 'Budget', 'offset': s, 'endPos': e, 'text': span_text})\n",
    "    if USE_SPACY:\n",
    "        doc = nlp(text)\n",
    "        for ent in doc.ents:\n",
    "            label = ent.label_\n",
    "            if label in ('GPE', 'LOC', 'ORG'):\n",
    "                et = 'Location'\n",
    "            elif label in ('DATE',):\n",
    "                et = 'Date'\n",
    "            elif label in ('TIME',):\n",
    "                et = 'Time'\n",
    "            elif label in ('MONEY',):\n",
    "                et = 'Budget'\n",
    "            else:\n",
    "                et = label\n",
    "            start, end = ent.start_char, ent.end_char-1\n",
    "            if any(existing['offset'] <= start <= existing['endPos'] or existing['offset'] <= end <= existing['endPos'] for existing in entities):\n",
    "                continue\n",
    "            if et in BLOCKLIST:\n",
    "                continue\n",
    "            entities.append({'category': et, 'offset': start, 'endPos': end, 'text': ent.text})\n",
    "        # dependency-based heuristics: look for numeric tokens attached to budget/price words\n",
    "        try:\n",
    "            for token in doc:\n",
    "                if token.like_num or token.ent_type_ == 'MONEY':\n",
    "                    head = token.head.lemma_.lower() if token.head is not None else ''\n",
    "                    left = token.nbor(-1).lemma_.lower() if token.i > 0 else ''\n",
    "                    right = token.nbor(1).lemma_.lower() if token.i < len(doc)-1 else ''\n",
    "                    cues = {'budget', 'price', 'cost', 'fare', 'costs', 'budgeted', 'pay', 'paying', 'expense', 'amount'}\n",
    "                    if head in cues or left in cues or right in cues or token.ent_type_ == 'MONEY':\n",
    "                        s = token.idx\n",
    "                        e = token.idx + len(token.text) - 1\n",
    "                        if not is_token_overlapping_spans(s, e, [(ds, de) for ds, de, _, _ in date_spans]):\n",
    "                            if not any(existing['offset'] == s and existing['endPos'] == e for existing in entities):\n",
    "                                entities.append({'category': 'Budget', 'offset': s, 'endPos': e, 'text': token.text})\n",
    "        except Exception:\n",
    "            pass\n",
    "    for m in re.finditer(r\"\\b([A-Z]{3})\\b\", text):\n",
    "        ctx = text[max(0, m.start()-20):m.end()+20].lower()\n",
    "        if 'airport' in ctx or 'from' in ctx or 'to' in ctx or 'arriv' in ctx or 'depart' in ctx:\n",
    "            start, end = m.start(), m.end()-1\n",
    "            if not any(s['offset'] <= start <= s['endPos'] for s in entities):\n",
    "                entities.append({'category': 'AirportCode', 'offset': start, 'endPos': end, 'text': m.group(1)})\n",
    "    for e in entities:\n",
    "        lab = e['category']\n",
    "        if lab.lower() in ('location', 'gpe', 'loc'):\n",
    "            e['category'] = 'Location'\n",
    "    return entities\n",
    "\n",
    "# === Helper to convert our internal entity form to the requested final form ===\n",
    "def to_final_entities(entity_list):\n",
    "    out = []\n",
    "    for ent in entity_list:\n",
    "        cat = ent.get('category') or ent.get('entity')\n",
    "        if not cat or cat in BLOCKLIST:\n",
    "            continue\n",
    "        start = ent.get('offset')\n",
    "        end = ent.get('endPos')\n",
    "        if start is None or end is None:\n",
    "            continue\n",
    "        length = end - start + 1\n",
    "        if length <= 0:\n",
    "            continue\n",
    "        out.append({'category': cat, 'offset': start, 'length': length})\n",
    "    return out\n",
    "\n",
    "# === Helper to Dedupe the found entities ===\n",
    "def dedupe_entities(entities):\n",
    "    \"\"\"Remove exact duplicates and resolve overlapping spans using a priority order.\n",
    "    Priority (higher first): Date > Budget > Time > Location > AirportCode > NumPassengers > others\n",
    "    \"\"\"\n",
    "    if not entities:\n",
    "        return []\n",
    "    \n",
    "    # === normalize keys first ===\n",
    "    items = []\n",
    "    for e in entities:\n",
    "        items.append({\n",
    "            'category': e.get('category'),\n",
    "            'start': int(e.get('offset')),\n",
    "            'end': int(e.get('endPos')),\n",
    "            'text': e.get('text')\n",
    "        })\n",
    "    # === remove exact duplicates first ===\n",
    "    uniq = []\n",
    "    seen = set()\n",
    "    for it in items:\n",
    "        key = (it['category'], it['start'], it['end'])\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        uniq.append(it)\n",
    "\n",
    "    # === resolve overlaps using priority ===\n",
    "    priority = {'Date': 6, 'Budget': 5, 'Time': 4, 'Location': 3, 'AirportCode': 2, 'NumPassengers': 1}\n",
    "\n",
    "    # === sort by start then by priority desc then by length desc ===\n",
    "    uniq.sort(key=lambda x: (x['start'], -priority.get(x['category'], 0), -(x['end']-x['start'])))\n",
    "\n",
    "    result = []\n",
    "    for cand in uniq:\n",
    "        overlap = False\n",
    "        for kept in result:\n",
    "            if not (cand['end'] < kept['start'] or cand['start'] > kept['end']):\n",
    "\n",
    "                # overlapping spans: keep the one with higher priority or longer span if same priority\n",
    "                p_c = priority.get(cand['category'], 0)\n",
    "                p_k = priority.get(kept['category'], 0)\n",
    "                if p_c > p_k:\n",
    "                    # replace kept with cand\n",
    "                    result.remove(kept)\n",
    "                    result.append(cand)\n",
    "                elif p_c == p_k:\n",
    "                    # same priority: keep longer span\n",
    "                    len_c = cand['end'] - cand['start']\n",
    "                    len_k = kept['end'] - kept['start']\n",
    "                    if len_c > len_k:\n",
    "                        result.remove(kept)\n",
    "                        result.append(cand)\n",
    "                overlap = True\n",
    "                break\n",
    "        if not overlap:\n",
    "            result.append(cand)\n",
    "\n",
    "    # === convert back to output format ===\n",
    "    out = []\n",
    "    for r in result:\n",
    "        out.append({\n",
    "            'category': r['category'],\n",
    "            'offset': r['start'],\n",
    "            'endPos': r['end'],\n",
    "            'text': r.get('text')\n",
    "        })\n",
    "    return out\n",
    "\n",
    "# === Load dataset robustly (file may be a dict with 'conversations' or a list of conversations) ===\n",
    "with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "conversations = dataset.get('conversations') if isinstance(dataset, dict) and 'conversations' in dataset else dataset\n",
    "\n",
    "\n",
    "# === If sklearn is available, attempt to load a persisted classifier + vectorizer if present. ===\n",
    "# DO NOT train or persist models from the notebook; only load existing artifacts.\n",
    "clf = None\n",
    "vectorizer = None\n",
    "if HAVE_SKLEARN:\n",
    "    try:\n",
    "        model_dir = Path(OUTPUT_FILE).parent / 'models'\n",
    "        clf_file = model_dir / 'intent_clf.joblib'\n",
    "        vect_file = model_dir / 'intent_vect.joblib'\n",
    "        if clf_file.exists() and vect_file.exists():\n",
    "            clf = joblib.load(clf_file)\n",
    "            vectorizer = joblib.load(vect_file)\n",
    "            print('✅ Loaded persisted intent classifier from', clf_file)\n",
    "        else:\n",
    "            print('ℹ️ sklearn available but no persisted classifier found; skipping training as requested')\n",
    "    except Exception as e:\n",
    "        print('⚠️ Failed to load persisted classifier:', e)\n",
    "        clf = None\n",
    "\n",
    "output = []\n",
    "\n",
    "\n",
    "# === Iterate conversations and turns ===\n",
    "for convo in (conversations or []):\n",
    "    turns = convo.get('turns') if isinstance(convo, dict) else []\n",
    "    for turn in (turns or []):\n",
    "        speaker = (turn.get('speaker') or turn.get('author') or '').lower()\n",
    "        if speaker != 'user':\n",
    "            continue\n",
    "        text = (turn.get('text') or '').strip()\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        # === determine intent: prefer frame/actions ===\n",
    "        intent = None\n",
    "        found = False\n",
    "        for fr in (turn.get('frames') or []):\n",
    "            for act in (fr.get('actions') or []):\n",
    "                act_type = (act.get('act') or act.get('type') or act.get('name') or '').lower().strip()\n",
    "                if not act_type:\n",
    "                    continue\n",
    "                if act_type in INTENT_MAP:\n",
    "                    intent = INTENT_MAP[act_type]\n",
    "                    found = True\n",
    "                    break\n",
    "                base = act_type.split('_')[0]\n",
    "                if base in INTENT_MAP:\n",
    "                    intent = INTENT_MAP[base]\n",
    "                    found = True\n",
    "                    break\n",
    "            if found:\n",
    "                break\n",
    "\n",
    "        # === classifier fallback ===\n",
    "        if not intent and clf is not None and vectorizer is not None:\n",
    "            try:\n",
    "                pred = clf.predict(vectorizer.transform([text]))\n",
    "                intent = pred[0]\n",
    "            except Exception:\n",
    "                intent = None\n",
    "\n",
    "        # === heuristic fallback on text if still unknown ===\n",
    "        if not intent:\n",
    "            txt = text.lower()\n",
    "            if any(k in txt for k in ['book', 'reserve', 'purchase', 'buy', 'ticket']):\n",
    "                intent = 'BookFlight'\n",
    "            elif any(k in txt for k in ['price', 'cost', 'fare', 'quote', 'how much', 'budget', '$']):\n",
    "                intent = 'RequestInfo'\n",
    "            elif any(k in txt for k in ['hello', 'hi', 'good morning', 'hey']):\n",
    "                intent = 'Greet'\n",
    "            elif any(k in txt for k in ['thanks', 'thank you']):\n",
    "                intent = 'ThankYou'\n",
    "            else:\n",
    "                intent = 'BookFlight'\n",
    "\n",
    "        # === entity extraction ===\n",
    "        utter_entities = []\n",
    "\n",
    "\n",
    "        # === prefer authoritative frame values when available ===\n",
    "        for fr in (turn.get('frames') or []):\n",
    "            candidates = []\n",
    "            if isinstance(fr.get('info'), list):\n",
    "                candidates = fr.get('info')\n",
    "            elif isinstance(fr.get('slots'), list):\n",
    "                candidates = fr.get('slots')\n",
    "            elif isinstance(fr.get('attributes'), list):\n",
    "                candidates = fr.get('attributes')\n",
    "            for info in (candidates or []):\n",
    "                slot = info.get('slot') or info.get('name') or info.get('key') or info.get('label')\n",
    "                value = info.get('value') or info.get('text') or info.get('values') or info.get('valueText')\n",
    "                if isinstance(value, list) and len(value) > 0:\n",
    "                    value = value[0]\n",
    "                if not slot or value is None:\n",
    "                    continue\n",
    "                # cat = ENTITY_MAP.get(slot.lower(), slot)\n",
    "                cat = (slot and slot.lower()) or slot\n",
    "                pos = None\n",
    "                try:\n",
    "                    pos = find_positions(text, value)\n",
    "                except Exception:\n",
    "                    pos = None\n",
    "                if pos:\n",
    "                    s, e = pos\n",
    "                    utter_entities.append({'category': cat, 'offset': s, 'endPos': e, 'text': str(value)})\n",
    "\n",
    "        # === supplement with text-based extraction ===\n",
    "        try:\n",
    "            extracted = extract_entities_from_text(text)\n",
    "        except Exception:\n",
    "            extracted = []\n",
    "        for e in (extracted or []):\n",
    "            category = e.get('category') or e.get('entity')\n",
    "            start = e.get('offset') if 'offset' in e else e.get('startPos')\n",
    "            end = e.get('endPos') if 'endPos' in e else e.get('end')\n",
    "            if category in BLOCKLIST:\n",
    "                continue\n",
    "            if start is None or end is None:\n",
    "                continue\n",
    "            dup = any(d['category'] == category and d['offset'] == start and d['endPos'] == end for d in utter_entities)\n",
    "            if not dup:\n",
    "                utter_entities.append({'category': category, 'offset': start, 'endPos': end, 'text': e.get('text')})\n",
    "\n",
    "        # === spaCy dependency-based budget heuristics ===\n",
    "        if USE_SPACY:\n",
    "            try:\n",
    "                doc = nlp(text)\n",
    "                for ent in doc.ents:\n",
    "                    if ent.label_.upper() == 'MONEY':\n",
    "                        s, e = ent.start_char, ent.end_char - 1\n",
    "                        if not any(existing['offset'] <= s <= existing['endPos'] or existing['offset'] <= e <= existing['endPos'] for existing in utter_entities):\n",
    "                            utter_entities.append({'category': 'Budget', 'offset': s, 'endPos': e, 'text': ent.text})\n",
    "                for token in doc:\n",
    "                    if token.like_num or token.ent_type_ == 'MONEY':\n",
    "                        head = token.head.lemma_.lower() if token.head is not None else ''\n",
    "                        left = token.nbor(-1).lemma_.lower() if token.i > 0 else ''\n",
    "                        right = token.nbor(1).lemma_.lower() if token.i < len(doc)-1 else ''\n",
    "                        cues = {'budget', 'price', 'cost', 'fare', 'costs', 'budgeted', 'pay', 'paying', 'expense', 'amount'}\n",
    "                        if head in cues or left in cues or right in cues or token.ent_type_ == 'MONEY':\n",
    "                            s = token.idx\n",
    "                            e = token.idx + len(token.text) - 1\n",
    "                            if not is_token_overlapping_spans(s, e, find_date_spans(text)):\n",
    "                                if not any(existing['offset'] == s and existing['endPos'] == e for existing in utter_entities):\n",
    "                                    utter_entities.append({'category': 'Budget', 'offset': s, 'endPos': e, 'text': token.text})\n",
    "            except Exception:\n",
    "                pass\n",
    "        # === dedupe and finalize entities ===\n",
    "        deduped = dedupe_entities(utter_entities)\n",
    "        final_entities = to_final_entities(deduped)\n",
    "        output.append({\n",
    "            'intent': intent,\n",
    "            'language': 'en-us',\n",
    "            'text': text,\n",
    "            'entities': final_entities\n",
    "        })\n",
    "\n",
    "# === save the simple array format ===\n",
    "Path(OUTPUT_FILE).parent.mkdir(parents=True, exist_ok=True)\n",
    "def normalize_utterance_text(t):\n",
    "    if not t:\n",
    "        return ''\n",
    "    import re\n",
    "    s = re.sub(r'\\s+', ' ', t).strip().lower()\n",
    "    return s\n",
    "# === dedupe utterances by (normalized_text, intent) preserving first occurrence ===\n",
    "seen_utts = set()\n",
    "deduped_output = []\n",
    "removed = 0\n",
    "for item in output:\n",
    "    key = (normalize_utterance_text(item.get('text', '')), item.get('intent'))\n",
    "    if key in seen_utts:\n",
    "        removed += 1\n",
    "        continue\n",
    "    seen_utts.add(key)\n",
    "    deduped_output.append(item)\n",
    "\n",
    "# === Save output (LUIS JSON) ===\n",
    "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(deduped_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f'✅ Wrote {len(deduped_output)} utterances to: {OUTPUT_FILE} (removed {removed} duplicates)')\n",
    "print(\"✅ LUIS JSON created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b99091b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"intents\": [\n",
      "{\n",
      "\"name\": \"BookFlight\"\n",
      "}\n",
      "],\n",
      "\"language\": \"en-us\",\n",
      "\"utterances\": [\n",
      "{\n",
      "\"text\": \"I'd like to book a trip to Atlantis from Caprica on Saturday, August 13, 2016 for 8 adults. I have a tight budget of 1700.\",\n",
      "\"intent\": \"BookFlight\",\n",
      "\"entities\": [\n",
      "{\n",
      "\"entity\": \"Budget\",\n",
      "\"startPos\": 117,\n",
      "\"endPos\": 120,\n",
      "\"text\": \"1700\"\n",
      "},\n",
      "{\n",
      "\"entity\": \"Date\",\n",
      "\"startPos\": 52,\n",
      "\"endPos\": 76,\n",
      "\"text\": \"Saturday, August 13, 2016\"\n",
      "},\n",
      "{\n",
      "\"entity\": \"CARDINAL\",\n",
      "\"startPos\": 82,\n",
      "\"endPos\": 82,\n",
      "\"text\": \"8\"\n",
      "}\n",
      "]\n",
      "},\n",
      "{\n",
      "\"text\": \"Yes, how about going to Neverland from Caprica on August 13, 2016 for 5 adults. For this trip, my budget would be 1900.\",\n",
      "\"intent\": \"BookFlight\",\n",
      "\"entities\": [\n",
      "{\n",
      "\"entity\": \"Budget\",\n",
      "\"startPos\": 114,\n",
      "\"endPos\": 117,\n",
      "\"text\": \"1900\"\n",
      "},\n",
      "{\n",
      "\"entity\": \"Location\",\n",
      "\"startPos\": 39,\n",
      "\"endPos\": 45,\n",
      "\"text\": \"Caprica\"\n",
      "},\n",
      "{\n",
      "\"entity\": \"Date\",\n",
      "\"startPos\": 50,\n",
      "\"endPos\": 64,\n",
      "\"text\": \"August 13, 2016\"\n",
      "},\n",
      "{\n",
      "\"entity\": \"CARDINAL\",\n",
      "\"startPos\": 70,\n",
      "\"endPos\": 70,\n",
      "\"text\": \"5\"\n",
      "}\n",
      "]\n",
      "},\n",
      "{\n",
      "\"text\": \"I have no flexibility for dates... but I can leave from Atlantis rather than Caprica. How about that?\",\n",
      "\"intent\": \"BookFlight\",\n",
      "\"entities\": []\n",
      "},\n",
      "{\n",
      "\"text\": \"I suppose I'll speak with my husband to see if we can choose other dates, and then I'll come back to you.Thanks for your help\",\n",
      "\"intent\": \"BookFlight\",\n",
      "\"entities\": []\n",
      "},\n",
      "{\n",
      "\"text\": \"Hello, I am looking to book a vacation from Gotham City to Mos Eisley for $2100.\",\n",
      "\"intent\": \"BookFlight\",\n",
      "\"entities\": [\n",
      "{\n",
      "\"entity\": \"Budget\",\n",
      "\"startPos\": 75,\n",
      "\"endPos\": 78,\n",
      "\"text\": \"2100\"\n",
      "},\n",
      "{\n",
      "\"entity\": \"Location\",\n",
      "\"startPos\": 44,\n",
      "\"endPos\": 54,\n",
      "\"text\": \"Gotham City\"\n",
      "}\n",
      "]\n",
      "},\n",
      "{\n",
      "\"text\": \"What about a trip from Gotham City to Neverland for the same budget?\",\n",
      "\"intent\": \"BookFlight\",\n",
      "\"entities\": [\n",
      "{\n",
      "\"entity\": \"Location\",\n",
      "\"startPos\": 23,\n",
      "\"endPos\": 33,\n",
      "\"text\": \"Gotham City\"\n",
      "}\n",
      "]\n",
      "},\n",
      "{\n",
      "\"text\": \"Would any packages to Mos Eisley be available if I increase my budget to $2500?\",\n",
      "\"intent\": \"BookFlight\",\n",
      "\"entities\": [\n",
      "{\n",
      "\"entity\": \"Budget\",\n",
      "\"startPos\": 74,\n",
      "\"endPos\": 77,\n",
      "\"text\": \"2500\"\n",
      "}\n",
      "]\n",
      "},\n",
      "{\n",
      "\"text\": \"You know what, I'd like to try and visit Neverland\",\n",
      "\"intent\": \"BookFlight\",\n",
      "\"entities\": []\n",
      "},\n",
      "{\n",
      "\"text\": \"Do you have any trips from Gotham City to Kobe for my original budget of $2100?\",\n",
      "\"intent\": \"BookFlight\",\n",
      "\"entities\": [\n",
      "{\n",
      "\"entity\": \"Budget\",\n",
      "\"startPos\": 74,\n",
      "\"endPos\": 77,\n",
      "\"text\": \"2100\"\n",
      "},\n",
      "{\n",
      "\"entity\": \"Location\",\n",
      "\"startPos\": 27,\n",
      "\"endPos\": 37,\n",
      "\"text\": \"Gotham City\"\n",
      "},\n",
      "{\n",
      "\"entity\": \"Location\",\n",
      "\"startPos\": 42,\n",
      "\"endPos\": 45,\n",
      "\"text\": \"Kobe\"\n",
      "}\n",
      "]\n",
      "},\n",
      "{\n",
      "\"text\": \"No, that's too far for me. I need a flight that leaves from Birmingham.\",\n",
      "\"intent\": \"BookFlight\",\n",
      "\"entities\": [\n",
      "{\n",
      "\"entity\": \"Location\",\n",
      "\"startPos\": 60,\n",
      "\"endPos\": 69,\n",
      "\"text\": \"Birmingham\"\n",
      "}\n",
      "]\n",
      "},\n",
      "{\n",
      "\"text\": \"How many days would I be in Kobe?\",\n",
      "\"intent\": \"BookFlight\",\n",
      "\"entities\": [\n",
      "{\n",
      "\"entity\": \"Location\",\n",
      "\"startPos\": 28,\n",
      "\"endPos\": 31,\n",
      "\"text\": \"Kobe\"\n",
      "}\n",
      "]\n",
      "},\n",
      "{\n",
      "\"text\": \"What would the price be if I shortened my trip by one day?\",\n",
      "\"intent\": \"BookFlight\",\n",
      "\"entities\": [\n",
      "{\n",
      "\"entity\": \"Date\",\n",
      "\"startPos\": 50,\n",
      "\"endPos\": 56,\n",
      "\"text\": \"one day\"\n",
      "}\n",
      "]\n",
      "},\n",
      "{\n",
      "\"text\": \"Ok, then I would like to purchase this package. What activities are included in this package?\",\n",
      "\"intent\": \"BookFlight\",\n",
      "\"entities\": []\n",
      "},\n",
      "{\n",
      "\"text\": \"Yes, I would like to book this package.\",\n",
      "\"intent\": \"BookFlight\",\n",
      "\"entities\": []\n",
      "},\n",
      "{\n",
      "\"text\": \"Hello there i am looking to go on a vacation with my family to Gotham City, can you help me?\",\n",
      "\"intent\": \"BookFlight\",\n",
      "\"entities\": [\n",
      "{\n",
      "\"entity\": \"Location\",\n",
      "\"startPos\": 63,\n",
      "\"endPos\": 73,\n",
      "\"text\": \"Gotham City\"\n",
      "}\n",
      "]\n",
      "},\n",
      "{\n",
      "\"text\": \"Not sure when we want to leave, but we are 12 kids and 5 adults\",\n",
      "\"intent\": \"BookFlight\",\n",
      "\"entities\": [\n",
      "{\n",
      "\"entity\": \"CARDINAL\",\n",
      "\"startPos\": 43,\n",
      "\"endPos\": 44,\n",
      "\"text\": \"12\"\n",
      "},\n",
      "{\n",
      "\"entity\": \"CARDINAL\",\n",
      "\"startPos\": 55,\n",
      "\"endPos\": 55,\n",
      "\"text\": \"5\"\n",
      "}\n",
      "]\n",
      "},\n",
      "{\n",
      "\"text\": \"yes i do, it is around $2200\",\n",
      "\"intent\": \"BookFlight\",\n",
      "\"entities\": [\n",
      "{\n",
      "\"entity\": \"Budget\",\n",
      "\"startPos\": 24,\n",
      "\"endPos\": 27,\n",
      "\"text\": \"2200\"\n",
      "}\n",
      "]\n",
      "},\n",
      "{\n",
      "\"text\": \"We are from Neverland\",\n",
      "\"intent\": \"BookFlight\",\n",
      "\"entities\": []\n",
      "},\n",
      "{\n",
      "\"text\": \"we can depart from Toronto\",\n",
      "\"intent\": \"BookFlight\",\n",
      "\"entities\": [\n",
      "{\n",
      "\"entity\": \"Location\",\n",
      "\"startPos\": 19,\n",
      "\"endPos\": 25,\n",
      "\"text\": \"Toronto\"\n",
      "}\n",
      "]\n",
      "},\n",
      "{\n",
      "\"text\": \"hmm what options would i have out of Toronto?\",\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load JSON file\n",
    "with open(OUTPUT_FILE, 'r') as file:\n",
    "    for i in range(250):  # Read first 150 lines\n",
    "        line = file.readline()\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6a4562",
   "metadata": {},
   "source": [
    "# Authoring the APP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd18a091",
   "metadata": {},
   "source": [
    "### Get an API key\n",
    "az cognitiveservices account keys list --resource-group <resource-group-name> --name <resource-name>\n",
    "\n",
    "\n",
    "az cognitiveservices account keys list --resource-group flyme_resource --name flyme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d2013d",
   "metadata": {},
   "source": [
    "### Create ConversationAnalysisClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab5fe7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.core\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.language.conversations import ConversationAnalysisClient\n",
    "\n",
    "endpoint = \"https://flyme.cognitiveservices.azure.com/\"\n",
    "credential = AzureKeyCredential(\"key1\")\n",
    "client = ConversationAnalysisClient(endpoint, credential)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086f7dae",
   "metadata": {},
   "source": [
    "### Create ConversationAuthoringClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4041fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.language.conversations.authoring import ConversationAuthoringClient\n",
    "\n",
    "endpoint = \"https://flyme.cognitiveservices.azure.com/\"\n",
    "credential = AzureKeyCredential(\"key1\")\n",
    "client = ConversationAuthoringClient(endpoint, credential)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cc9a01",
   "metadata": {},
   "source": [
    "### Create a client with an Azure Active Directory Credential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f735ef27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.language.conversations import ConversationAnalysisClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "client = ConversationAnalysisClient(endpoint=\"https://flyme.cognitiveservices.azure.com/\", credential=credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036a4999",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # pip show azure-ai-textanalytics\n",
    "    # pip install azure-ai-textanalytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3eeaba",
   "metadata": {},
   "source": [
    "### Example using DefaultAzureCredential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c5b5273",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'https://flyme.cognitiveservices.azure.com/'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mazure\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlanguage\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconversations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConversationAnalysisClient\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# get secrets\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# clu_endpoint = os.environ[\"AZURE_CONVERSATIONS_ENDPOINT\"]\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m clu_endpoint = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# clu_key = os.environ[\"AZURE_CONVERSATIONS_KEY\"]\u001b[39;00m\n\u001b[32m     10\u001b[39m clu_key = os.environ[credential]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:716\u001b[39m, in \u001b[36m__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'https://flyme.cognitiveservices.azure.com/'"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import os\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.language.conversations import ConversationAnalysisClient\n",
    "\n",
    "# get secrets\n",
    "# clu_endpoint = os.environ[\"AZURE_CONVERSATIONS_ENDPOINT\"]\n",
    "clu_endpoint = os.environ[endpoint]\n",
    "# clu_key = os.environ[\"AZURE_CONVERSATIONS_KEY\"]\n",
    "clu_key = os.environ[credential]\n",
    "# project_name = os.environ[\"AZURE_CONVERSATIONS_PROJECT_NAME\"]\n",
    "project_name = os.environ[booking_travel]\n",
    "# deployment_name = os.environ[\"AZURE_CONVERSATIONS_DEPLOYMENT_NAME\"]\n",
    "deployment_name = os.environ[luismodel]\n",
    "\n",
    "# analyze quey\n",
    "client = ConversationAnalysisClient(clu_endpoint, AzureKeyCredential(clu_key))\n",
    "with client:\n",
    "    query = \"Hi there! So, between September 7 and 27 I would like to see what is available from Curitiba to Mexico City. My budget is around 1900 dollars and I will be traveling with my wife and two kids. Can you help me with that?\"\n",
    "    result = client.analyze_conversation(\n",
    "        task={\n",
    "            \"kind\": \"Conversation\",\n",
    "            \"analysisInput\": {\n",
    "                \"conversationItem\": {\n",
    "                    \"participantId\": \"1\",\n",
    "                    \"id\": \"1\",\n",
    "                    \"modality\": \"text\",\n",
    "                    \"language\": \"en\",\n",
    "                    \"text\": query\n",
    "                },\n",
    "                \"isLoggingEnabled\": False\n",
    "            },\n",
    "            \"parameters\": {\n",
    "                \"projectName\": project_name,\n",
    "                \"deploymentName\": deployment_name,\n",
    "                \"verbose\": True\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "# view result\n",
    "print(\"query: {}\".format(result[\"result\"][\"query\"]))\n",
    "print(\"project kind: {}\\n\".format(result[\"result\"][\"prediction\"][\"projectKind\"]))\n",
    "\n",
    "print(\"top intent: {}\".format(result[\"result\"][\"prediction\"][\"topIntent\"]))\n",
    "print(\"category: {}\".format(result[\"result\"][\"prediction\"][\"intents\"][0][\"category\"]))\n",
    "print(\"confidence score: {}\\n\".format(result[\"result\"][\"prediction\"][\"intents\"][0][\"confidenceScore\"]))\n",
    "\n",
    "print(\"entities:\")\n",
    "for entity in result[\"result\"][\"prediction\"][\"entities\"]:\n",
    "    print(\"\\ncategory: {}\".format(entity[\"category\"]))\n",
    "    print(\"text: {}\".format(entity[\"text\"]))\n",
    "    print(\"confidence score: {}\".format(entity[\"confidenceScore\"]))\n",
    "    if \"resolutions\" in entity:\n",
    "        print(\"resolutions\")\n",
    "        for resolution in entity[\"resolutions\"]:\n",
    "            print(\"kind: {}\".format(resolution[\"resolutionKind\"]))\n",
    "            print(\"value: {}\".format(resolution[\"value\"]))\n",
    "    if \"extraInformation\" in entity:\n",
    "        print(\"extra info\")\n",
    "        for data in entity[\"extraInformation\"]:\n",
    "            print(\"kind: {}\".format(data[\"extraInformationKind\"]))\n",
    "            if data[\"extraInformationKind\"] == \"ListKey\":\n",
    "                print(\"key: {}\".format(data[\"key\"]))\n",
    "            if data[\"extraInformationKind\"] == \"EntitySubtype\":\n",
    "                print(\"value: {}\".format(data[\"value\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CompViz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
